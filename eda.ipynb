{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e0d6751-c093-4c78-ae49-fc4107852e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T01:37:20.279215887Z",
     "start_time": "2024-06-24T01:37:20.228812135Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:06:10.026714214Z",
     "start_time": "2024-04-29T11:06:09.984375950Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "import numpy as np \n",
    "import joblib\n",
    "import re  \n",
    "import jieba \n",
    "import json\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from pandasql import sqldf\n",
    "from preprocessing import extract_versions, extract_models, WordCut\n",
    "\n",
    "from datasets import Dataset \n",
    "from datasets import Features, ClassLabel, Sequence, Value\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73929db28d1c2b01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score, classification_report\n",
    "from typing import Any, List, Dict, Union\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain.chains import RetrievalQA\n",
    "import sqlalchemy\n",
    "sqlalchemy.types\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "import inspect\n",
    "from langchain.chains.retrieval_qa import base\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "RetrievalQA.from_chain_type\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from langchain.retrievers import BM25Retriever,\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import chromadb\n",
    "from langchain_bailian import Bailian\n",
    "chroma_client = chromadb.Client()\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.llms import AzureOpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain_openai import AzureOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "ChatOpenAI.bind_tools\n",
    "from langchain.llms import Tongyi\n",
    "\n",
    "llm = Tongyi(dashscope_api_key=DASHSCOPE_API_KEY)\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import sequential, llm\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain import LLMChain\n",
    "from langchain.schema import Messages\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain.chains.sql_database import query\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.chains import llm\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "from sqlalchemy import create_engine, \n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain.chains.sql_database import query\n",
    "from sqlalchemy.schema import CreateTable\n",
    "from langchain.chains.openai_tools import create_extraction_chain_pydantic\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.chains import SimpleChain\n",
    "from sqlalchemy import create_engine, inspect\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "esults = collection.query(\n",
    "    query_texts=[\"This is a query document\"],\n",
    "    n_results=2\n",
    ")\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "docs = [\"平台ID\", \"商品id\", \"商品编码\", \"商品分类\", \"商品名字\", \"版本\", \"商品型号\", \"primary_key\"]\n",
    "encoder = HuggingFaceEmbeddings(model_name=\"/data/dataset/huggingface/hub/bge-large-zh-v1.5\", \n",
    "                                   encode_kwargs = {'normalize_embeddings': True},\n",
    "                                   model_kwargs={'device': 'cuda:0'})\n",
    "store = FAISS.from_documents([Document(page_content=line, metadata={\"id\": id})\n",
    " for id, line in enumerate(docs)], embedding=encoder)\n",
    "store.similarity_search_with_relevance_scores(\"商品分类\", k=3, filter=filter)\n",
    "import os\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# 创建client\n",
    "client = chromadb.HttpClient(\n",
    "                             host=\"192.168.111.48\",  # 你的服务器ip\n",
    "                             port=8000,  # 你的服务器端口\n",
    "                             settings=Settings(allow_reset=True)\n",
    "                             )\n",
    "import mysql.connector\n",
    "\n",
    "# 创建连接\n",
    "conn = mysql.connector.connect\n",
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history_for_chain = ChatMessageHistory()\n",
    "\n",
    "conversational_agent_executor = RunnableWithMessageHistory\n",
    "\n",
    "store = Chroma.from_documents([Document(page_content=line, metadata={\"id\": id,\n",
    "                                                                    \"washing\": \"washing\" in filter[line],\n",
    "                                                                    \"mopping\": \"mopping\" in filter[line],\n",
    "                                                                    \"sweeping\": \"sweeping\" in filter[line],\n",
    "                                                                   })\n",
    " for id, line in enumerate(schema_set)], embedding=encoder, collection_name=\"my_collection\")\n",
    "store._collection.get()\n",
    "aa = store.as_retriever()\n",
    "aa.invoke()\n",
    "aa.get_relevant_documents\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "engine = create_engine(\"sqlite:///titanic.db\")\n",
    "df_all.iloc[:10, :20].to_sql(\"titanic\", engine, index=False)\n",
    "SQLDatabase(engine=engine, indexes_in_table_info=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe60c9f2d78278fa"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a46b3955067fa79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:06:10.437085278Z",
     "start_time": "2024-04-29T11:06:10.431104945Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandasql查询函数需要的环境\n",
    "pysqldf = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0377b93d-b87b-4aa0-8181-fbdaef8edd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(question, all_model_list, wc):\n",
    "    original_question = question\n",
    "    model_list, question = extract_models(question, all_model_list)\n",
    "    version_list, question = extract_versions(question)\n",
    "    key_words = wc.cut(question)\n",
    "    key_words = [i for i in key_words if ((i.find(\"model\")<0) and (i.find(\"version\")<0))]\n",
    "    return {\"question\": original_question, \"model\": model_list, \"version\": version_list, \"keywords\": key_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb2287519920465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:06:10.760443187Z",
     "start_time": "2024-04-29T11:06:10.743542231Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 原始数据处理\n",
    "def format_model(x):\n",
    "    model_list = x.split(',')\n",
    "    model_list = [i.strip().lower().replace(\" \", \"\") for i in model_list]\n",
    "    new_list = [model_list[0]]\n",
    "    i = 1\n",
    "    while i < len(model_list):\n",
    "        if (i != len(model_list) - 1) and (model_list[i-1] == model_list[i]):\n",
    "            new_list.append(model_list[i]+model_list[i+1])\n",
    "            if i < len(model_list) - 1:\n",
    "                i += 2\n",
    "            else:\n",
    "                break\n",
    "        elif (i != len(model_list) - 1) and (model_list[i-1] != model_list[i]):\n",
    "            new_list.append(model_list[i])\n",
    "            i += 1\n",
    "        elif (model_list[i] == \"上下水\") or (model_list[i] == \"air\"):\n",
    "            for j in range(len(new_list)):\n",
    "                if model_list[i-1] == new_list[j]:\n",
    "                    new_list.pop(j)\n",
    "                    break\n",
    "            new_list.append(model_list[i-1]+model_list[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            new_list.append(model_list[i])\n",
    "            break\n",
    "    return new_list\n",
    "\n",
    "def format_all_models(x, dim_df):\n",
    "    new_list = []\n",
    "    for i in x:\n",
    "        if i.find(\"全型号\") >= 0:\n",
    "            end_idx = i.find(\"全型号\")\n",
    "            name = i[:end_idx]\n",
    "            new_list += [j for j in dim_df[dim_df['cat_name'] == name].model.tolist() if j not in x]\n",
    "        else:\n",
    "            new_list.append(i)\n",
    "    return new_list\n",
    "\n",
    "def format_series(x, dim_df):\n",
    "    def contains_chinese(s):\n",
    "        return re.search('[\\u4e00-\\u9fff]', s) is not None\n",
    "    new_list = []\n",
    "    for i in x:\n",
    "        if i.find(\"系列\") >= 0:\n",
    "            end_idx = i.find(\"系列\")\n",
    "            name = i[:end_idx]\n",
    "            new_list += [j for j in dim_df[(dim_df.model.str.find(name)>=0) & (\n",
    "                dim_df.model.apply(lambda x: not contains_chinese(x)))].model.tolist() if j not in x]\n",
    "            new_list += [i]\n",
    "        else:\n",
    "            new_list.append(i)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c313d44fac87ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:06:11.584453395Z",
     "start_time": "2024-04-29T11:06:11.580777532Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_gt(x):\n",
    "    if str(x) == \"nan\":\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x.split(\",\"))  \n",
    "    \n",
    "def find_non_chinese_substrings(s):\n",
    "    # 正则表达式解释：\n",
    "    # [^\\u4e00-\\u9fff\\W]+ 匹配非中文字符和非ASCII标点的连续字符\n",
    "    # 但这样会排除空格，所以我们需要允许空格存在\n",
    "    # 我们使用(?:[^\\u4e00-\\u9fff\\W]| )+ 来实现这一点，(?:) 是非捕获组，用于匹配模式但不作为捕获结果返回\n",
    "    # [^\\u4e00-\\u9fff\\W] 匹配非中文且非标点的字符，| 表示或，空格 ' ' 被显式允许\n",
    "    pattern = r'(?:[^\\u4e00-\\u9fff\\W]| )+'\n",
    "    \n",
    "    # 使用findall方法查找所有匹配项\n",
    "    matches = re.findall(pattern, s)\n",
    "    \n",
    "    # 过滤掉只包含空格的字符串\n",
    "    matches = [match for match in matches if not match.isspace()]\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def clean_string(s):\n",
    "    s = s.replace(\" \", \"\").lower()\n",
    "    return s\n",
    "\n",
    "def find_model(x, all_model_list):\n",
    "    x = x.replace(\"\\n\", \"\") \n",
    "    x = find_non_chinese_substrings(x)\n",
    "    result = [clean_string(s) for s in x]\n",
    "    return [model for model in all_model_list if model in result]\n",
    "\n",
    "def find_cat(x, all_cat_list):\n",
    "    return [name for name in all_cat_list if name in x]   \n",
    "\n",
    "def filter_model(x, model_list):\n",
    "    x = x.split(\",\")\n",
    "    for model in model_list:\n",
    "        if model in x:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_error_with_reason(a):\n",
    "    # 第一次匹配“错误xxx”\n",
    "    pattern1 = r\"错误\\s*\\d+\"\n",
    "    matches1 = re.findall(pattern1, a)\n",
    "    \n",
    "    # 第二次匹配“错误原因xxx”\n",
    "    pattern2 = r\"错误原因\\s*\\d+\"\n",
    "    matches2 = re.findall(pattern2, a)\n",
    "\n",
    "    # 合并两次匹配的结果\n",
    "    matches = matches1 + matches2\n",
    "    \n",
    "    return [name.replace(\" \", \"\").replace(\"原因\", \"\") for name in matches]\n",
    "\n",
    "def filter_reason(x, query_reason_list):\n",
    "    reason_list = find_error_with_reason(x)\n",
    "    for name in query_reason_list:\n",
    "        if name in reason_list:\n",
    "            return True \n",
    "    return False\n",
    "\n",
    "def transform_model_name(x, all_model_list):\n",
    "    x = x.replace(\"\\n\", \"\") \n",
    "    candidates = find_non_chinese_substrings(x)\n",
    "    for name in candidates:\n",
    "        cleaned_name = clean_string(name)\n",
    "        for model in all_model_list:\n",
    "            if cleaned_name == model:\n",
    "                x = x.replace(name, model)\n",
    "                break\n",
    "    return x \n",
    "\n",
    "def remove_model_name(x, all_model_list):\n",
    "    x = x.replace(\"\\n\", \"\") \n",
    "    candidates = find_non_chinese_substrings(x)\n",
    "    for name in candidates:\n",
    "        if clean_string(name) in all_model_list:\n",
    "            x = x.replace(name, \"\")\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1873856a-15e0-4182-b511-4c30b5b77a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_tagging(texts, labels):\n",
    "    bio_words = []\n",
    "    bio_tags = []\n",
    "    texts = texts.lower()\n",
    "    for i, char in enumerate(texts):\n",
    "        tag = \"O\"  # 默认为 Outside\n",
    "\n",
    "        for entity_type, spans in labels.items():\n",
    "            for span in spans.values():\n",
    "                for ind in span:\n",
    "                    if i == int(ind[0]):\n",
    "                        tag = \"B-\" + entity_type\n",
    "                        break\n",
    "                    elif int(ind[0]) < i <= int(ind[1]):\n",
    "                        tag = \"I-\" + entity_type\n",
    "                        break\n",
    "\n",
    "        bio_words.append(char)\n",
    "        bio_tags.append(tag)\n",
    "\n",
    "    return bio_words, bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24087d4-51c4-4b6d-a875-1c3430aecfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义NER数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89099c8b5609671b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T01:56:42.794901015Z",
     "start_time": "2024-04-28T01:56:41.663211436Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"/data/dataset/kefu/database_with_emb20240315.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79693cce64b935cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T01:59:36.896064386Z",
     "start_time": "2024-04-28T01:59:35.736723322Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oot = pd.read_excel(\"/data/dataset/kefu/国内客服助手（生产环境）_中转栈.xlsx\")\n",
    "oot = oot.rename(columns={\"编号\": \"qa_id\",\n",
    " \"问题\": \"question\",\n",
    " \"回复1\": \"answer1_all\",\n",
    " \"回复1标题\": \"answer1\",\n",
    " \"回复2\": \"answer2_all\",\n",
    " \"回复2标题\": \"answer2\",\n",
    " \"是否解决\": \"if_solved\",\n",
    " \"提问者\": \"requester\",\n",
    " \"提问者所在组别\": \"requester_group\",\n",
    " \"提问日期\": \"request_time\",\n",
    " \"类型\": \"data_type\", \n",
    " \"正确回复\": \"gt_answer\"})\n",
    "oot = oot.drop([\"回复1附件\", \"回复2附件\", \"提问日期(供统计用)\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b88123839a502a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T01:59:41.213093995Z",
     "start_time": "2024-04-28T01:59:40.896097394Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# oot = oot[oot.if_solved.notnull()]\n",
    "# oot = oot[oot.answer1_all.notnull()]\n",
    "oot[\"if_solved\"] = oot[\"if_solved\"].map({\"已解决\": 1, \"未解决\": 0})\n",
    "oot.loc[oot.qa_id==\"ICASK202308010583\", \"gt_answer\"] = \"ICWIKI202307243975\"\n",
    "oot.loc[oot.qa_id==\"ICASK202308010582\", \"gt_answer\"] = \"ICWIKI202308210081\"\n",
    "# oot = oot[oot['gt_answer'].str.find(\"ICW\")>=0]\n",
    "oot = oot.rename(columns={\"gt_answer\": \"gt_qa_id\"})\n",
    "oot = oot[oot.question.notnull()]\n",
    "\n",
    "temp = oot.copy()\n",
    "temp[\"gt_qa_id\"] = temp[\"gt_qa_id\"].astype(str).apply(lambda x: x.split(','))\n",
    "temp_exploded = temp.explode(\"gt_qa_id\")\n",
    "temp_right = df2[['qa_id', \n",
    "                               'question', \n",
    "                               'answer', \n",
    "                               'model', \n",
    "                               'qa_type', \n",
    "                               'model_list', \n",
    "                               'cat_name']].copy()\n",
    "query = f\"\"\"\n",
    "select \n",
    "    a.*\n",
    "    ,b.question as question_kg\n",
    "    ,b.answer as answer_kg\n",
    "    ,b.model as model\n",
    "    ,b.qa_type\n",
    "    ,b.model_list\n",
    "    ,b.cat_name\n",
    "from \n",
    "    temp_exploded a \n",
    "left join \n",
    "    temp_right b\n",
    "on \n",
    "    a.gt_qa_id = b.qa_id\n",
    "\"\"\"\n",
    "\n",
    "# 使用pysqldf执行SQL查询\n",
    "temp_exploded = pysqldf(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71096899a84dc835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T01:59:41.457923317Z",
     "start_time": "2024-04-28T01:59:41.336686126Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select \n",
    "    qa_id\n",
    "    ,group_concat(question_kg) as question_kg\n",
    "    ,group_concat(answer_kg) as answer_kg\n",
    "    ,group_concat(model) as model\n",
    "    ,group_concat(qa_type) as qa_type\n",
    "    ,group_concat(model_list) as model_list\n",
    "    ,group_concat(cat_name) as cat_name\n",
    "from \n",
    "    temp_exploded\n",
    "group by \n",
    "    qa_id\n",
    "\"\"\"\n",
    "\n",
    "# 使用pysqldf执行SQL查询\n",
    "temp_exploded = pysqldf(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce8d032a1415a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T01:59:42.165957893Z",
     "start_time": "2024-04-28T01:59:41.950818359Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select \n",
    "    a.*\n",
    "    ,b.question_kg\n",
    "    ,b.answer_kg\n",
    "    ,b.model\n",
    "    ,b.qa_type\n",
    "    ,b.model_list\n",
    "    ,b.cat_name\n",
    "from \n",
    "    oot a \n",
    "left join \n",
    "    temp_exploded b\n",
    "on\n",
    "    a.qa_id = b.qa_id\n",
    "\"\"\"\n",
    "\n",
    "# 使用pysqldf执行SQL查询\n",
    "oot = pysqldf(query)\n",
    "\n",
    "oot['gt_num'] = oot['gt_qa_id'].astype(str).apply(lambda x: count_gt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17bd1de0082896ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T02:14:55.794165425Z",
     "start_time": "2024-04-23T02:14:55.770553239Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/dataset/kefu/oot20240422.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(oot[[\"qa_id\", \n",
    "     \"question\",\t\n",
    "     \"gt_qa_id\",\n",
    "     \"gt_num\",\n",
    "     \"question_kg\",\n",
    "     \"answer_kg\",\n",
    "     \"model\",\n",
    "     \"qa_type\",\n",
    "     \"model_list\",\n",
    "     \"cat_name\",\n",
    "     ]], \"/data/dataset/kefu/oot20240422.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed91dc93dc98b4e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T02:00:04.604315108Z",
     "start_time": "2024-04-28T02:00:04.576956929Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oot = joblib.load(\"/data/dataset/kefu/oot20240422.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e7cf7b82cdbcb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:06:26.894658940Z",
     "start_time": "2024-04-29T11:06:26.884394630Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_df = pd.read_csv(\"/data/dataset/kefu/dim_df20240315.csv\")\n",
    "all_model_list = dim_df.model.tolist()\n",
    "all_cat_list = dim_df.cat_name.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c02e41c40f9721ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:06:49.226822526Z",
     "start_time": "2024-04-30T06:06:49.187334074Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wc = WordCut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd6a12216117425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:06:51.879045616Z",
     "start_time": "2024-04-30T06:06:51.197991383Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# oot[\"keywords\"] = oot[\"question\"].apply(\n",
    "#     lambda x: extract_keywords(x, all_model_list, wc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7936940469990b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T04:02:07.456290967Z",
     "start_time": "2024-04-23T04:02:07.418266531Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# oot[oot.version_keywords.apply(lambda x: len(x)>0)][[\"question\", \"keywords\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28804a45-c17c-4e16-b1b0-667c9b161651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义知识库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3a6dd7feda66898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:31.559771881Z",
     "start_time": "2024-04-28T05:40:31.382932816Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sweeping = pd.read_excel(\"/data/dataset/kefu/产品知识整理资料.xlsx\", sheet_name=\"扫地机\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65eb06f7-0a38-488b-9715-bf3f1d124039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:31.719786114Z",
     "start_time": "2024-04-28T05:40:31.684857469Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sweeping.loc[df_sweeping[\"上市时间\"]==45323, \"上市时间\"] = datetime.datetime(2024, 2, 1, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef2cdef4-1219-49a0-bbda-f88f84b80294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:32.147602568Z",
     "start_time": "2024-04-28T05:40:31.925684954Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mopping = pd.read_excel(\"/data/dataset/kefu/产品知识整理资料.xlsx\", sheet_name=\"洗地机\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fff9bc0-73f0-4052-89f4-3c4b6af69d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:32.394020413Z",
     "start_time": "2024-04-28T05:40:32.276702462Z"
    }
   },
   "outputs": [],
   "source": [
    "df_washing = pd.read_excel(\"/data/dataset/kefu/产品知识整理资料.xlsx\", sheet_name=\"洗衣机\")\n",
    "df_washing = df_washing.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20a23557-c6ca-4d13-96fa-c8e459064853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:35.379055088Z",
     "start_time": "2024-04-28T05:40:35.368009814Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in [\"商品编码\", \"平台ID\", \"商品id\"]:\n",
    "    df_sweeping[col] = df_sweeping[col].astype(str)\n",
    "    df_sweeping.loc[df_sweeping[col]=='nan', col] = np.nan\n",
    "    df_mopping[col] = df_mopping[col].astype(str)\n",
    "    df_mopping.loc[df_mopping[col]=='nan', col] = np.nan\n",
    "    df_washing[col] = df_washing[col].astype(np.int64).astype(str)\n",
    "    df_washing.loc[df_washing[col]=='nan', col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbbd7b7a15be929e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:37.584660528Z",
     "start_time": "2024-04-28T05:40:37.576537848Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 73)\n",
      "(7, 87)\n",
      "(14, 91)\n"
     ]
    }
   ],
   "source": [
    "print(df_washing.shape)\n",
    "print(df_mopping.shape)\n",
    "print(df_sweeping.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12f70f61b1159bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:39.943262001Z",
     "start_time": "2024-04-28T05:40:39.734765199Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_washing.to_csv(\"/data/dataset/kefu/washing.csv\", index=None)\n",
    "# df_mopping.to_csv(\"/data/dataset/kefu/mopping.csv\", index=None)\n",
    "# df_sweeping.to_csv(\"/data/dataset/kefu/sweeping.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec4a52a475a53ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:42.897943465Z",
     "start_time": "2024-04-28T05:40:42.872806032Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_washing, df_mopping, df_sweeping], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af2578d6794abf5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:44.714308376Z",
     "start_time": "2024-04-28T05:40:44.701231577Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>商品编码</th>\n",
       "      <th>平台ID</th>\n",
       "      <th>商品id</th>\n",
       "      <th>商品型号</th>\n",
       "      <th>商品名字</th>\n",
       "      <th>商品分类</th>\n",
       "      <th>商品链接</th>\n",
       "      <th>平台</th>\n",
       "      <th>店铺名称</th>\n",
       "      <th>服务别名</th>\n",
       "      <th>...</th>\n",
       "      <th>主刷转速</th>\n",
       "      <th>有无虚拟墙</th>\n",
       "      <th>电源线长</th>\n",
       "      <th>是否支持银离子抑菌</th>\n",
       "      <th>功能</th>\n",
       "      <th>清扫模式</th>\n",
       "      <th>有无地毯自动增压模式</th>\n",
       "      <th>清扫路线</th>\n",
       "      <th>是否支持自动回洗拖布</th>\n",
       "      <th>保修期</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>701135838300</td>\n",
       "      <td>701135838300</td>\n",
       "      <td>701135838300</td>\n",
       "      <td>H1</td>\n",
       "      <td>石头12公斤分子筛洗烘一体H1家用烘干全自动滚筒洗衣机大容量除菌</td>\n",
       "      <td>洗衣机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=701135838...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>735903284970</td>\n",
       "      <td>735903284970</td>\n",
       "      <td>735903284970</td>\n",
       "      <td>H1 Neo</td>\n",
       "      <td>【新品】石头分子筛12KG洗烘一体家用全自动滚筒洗衣机H1Neo 除菌</td>\n",
       "      <td>洗衣机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=735903284...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739010441482</td>\n",
       "      <td>739010441482</td>\n",
       "      <td>739010441482</td>\n",
       "      <td>M1</td>\n",
       "      <td>【新品】石头1kg内衣裤迷你分子筛洗烘一体全自动滚筒洗衣机M1</td>\n",
       "      <td>洗衣机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=739010441...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5014561169968</td>\n",
       "      <td>714853608871</td>\n",
       "      <td>5014561169968</td>\n",
       "      <td>A10 Ultra</td>\n",
       "      <td>【全屋清洁】石头洗地机A10 Ultra家用除菌除螨贴边吸拖洗一体机</td>\n",
       "      <td>洗地机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=714853608...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5213854695645</td>\n",
       "      <td>725413870652</td>\n",
       "      <td>5213854695645</td>\n",
       "      <td>A10 UltraE</td>\n",
       "      <td>【一机多用】石头洗地机A10 UltraE家用除菌除螨贴边吸拖洗一体机</td>\n",
       "      <td>洗地机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=725413870...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            商品编码          平台ID           商品id        商品型号  \\\n",
       "0   701135838300  701135838300   701135838300          H1   \n",
       "1   735903284970  735903284970   735903284970      H1 Neo   \n",
       "2   739010441482  739010441482   739010441482          M1   \n",
       "3  5014561169968  714853608871  5014561169968   A10 Ultra   \n",
       "4  5213854695645  725413870652  5213854695645  A10 UltraE   \n",
       "\n",
       "                                  商品名字 商品分类  \\\n",
       "0     石头12公斤分子筛洗烘一体H1家用烘干全自动滚筒洗衣机大容量除菌  洗衣机   \n",
       "1  【新品】石头分子筛12KG洗烘一体家用全自动滚筒洗衣机H1Neo 除菌  洗衣机   \n",
       "2      【新品】石头1kg内衣裤迷你分子筛洗烘一体全自动滚筒洗衣机M1  洗衣机   \n",
       "3   【全屋清洁】石头洗地机A10 Ultra家用除菌除螨贴边吸拖洗一体机  洗地机   \n",
       "4  【一机多用】石头洗地机A10 UltraE家用除菌除螨贴边吸拖洗一体机  洗地机   \n",
       "\n",
       "                                                商品链接  平台     店铺名称  服务别名  ...  \\\n",
       "0  https://detail.tmall.com/item.htm?id=701135838...  淘宝  石头电器旗舰店   NaN  ...   \n",
       "1  https://detail.tmall.com/item.htm?id=735903284...  淘宝  石头电器旗舰店   NaN  ...   \n",
       "2  https://detail.tmall.com/item.htm?id=739010441...  淘宝  石头电器旗舰店   NaN  ...   \n",
       "3  https://detail.tmall.com/item.htm?id=714853608...  淘宝  石头电器旗舰店   NaN  ...   \n",
       "4  https://detail.tmall.com/item.htm?id=725413870...  淘宝  石头电器旗舰店   NaN  ...   \n",
       "\n",
       "  主刷转速 有无虚拟墙 电源线长 是否支持银离子抑菌   功能 清扫模式 有无地毯自动增压模式 清扫路线 是否支持自动回洗拖布  保修期  \n",
       "0  NaN   NaN  NaN       NaN  NaN  NaN        NaN  NaN        NaN  NaN  \n",
       "1  NaN   NaN  NaN       NaN  NaN  NaN        NaN  NaN        NaN  NaN  \n",
       "2  NaN   NaN  NaN       NaN  NaN  NaN        NaN  NaN        NaN  NaN  \n",
       "3  NaN   NaN  NaN       NaN  NaN  NaN        NaN  NaN        NaN  NaN  \n",
       "4  NaN   NaN  NaN       NaN  NaN  NaN        NaN  NaN        NaN  NaN  \n",
       "\n",
       "[5 rows x 210 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8d0d3375d35e365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T05:40:45.497813123Z",
     "start_time": "2024-04-28T05:40:45.493861648Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_all.to_csv(\"/data/dataset/kefu/knowledge.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "358bb9b2-f678-40c6-8533-2eb104aaac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"/data/dataset/kefu/knowledge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5a7f4-a672-4451-889b-0570d5cef0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用csv2qa.py从模板收集数据表格问答数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6a1c26-b8b8-44da-93fc-aa4c788f4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_file='/data/dataset/kefu/gen_different_keywords_different_models.jsonl'):\n",
    "    df = []\n",
    "    # 打开文件进行读取\n",
    "    with open(json_file, 'r') as file:\n",
    "        # 逐行读取\n",
    "        for line in file:\n",
    "            # 将每行的内容从JSON字符串转换为Python字典\n",
    "            data = json.loads(line.strip())\n",
    "            # 现在可以处理这个字典了\n",
    "            df.append(data)\n",
    "    return df \n",
    "\n",
    "def extract_json(x):\n",
    "    result = {\"template\": x[\"template\"]}\n",
    "    result[\"replace\"] = x[\"gen\"][\"replace\"]\n",
    "    return json.dumps(result, ensure_ascii=False)\n",
    "\n",
    "file_list = ['/data/dataset/kefu/gen_same_keywords_for_models.jsonl',\n",
    "             '/data/dataset/kefu/gen_different_keywords_different_models.jsonl']\n",
    "df = []\n",
    "for file in file_list:\n",
    "    temp = pd.DataFrame(read_json(file))\n",
    "    temp['type'] = file.split('/')[-1].split(\".\")[0]\n",
    "    df.append(temp)\n",
    "df = pd.concat(df, axis=0).reset_index(drop=True)\n",
    "\n",
    "df[\"final_prompt\"] = df.apply(lambda x: extract_json(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74feecaf-ff24-4d04-bf90-c1eb9926266d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0081148d-9e90-416d-9c53-1f36640a7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模板收集数据的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f70cccafc28cb5cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T07:10:11.693120013Z",
     "start_time": "2024-04-29T07:10:11.646822268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "输入json含义如下：\n",
    "1. template是一个问句，问句里面用[]符号括起来的部分是命名实体\n",
    "2. replace是上述每个命名实体的具体值\n",
    "\n",
    "请你按以下步骤操作：\n",
    "1. 将template里面的每个命名实体替换成replace里面对应的值\n",
    "2. 把这个句子改写得语言通顺或者完全更改template的原始句式且通顺，但是请保持“问询词”开头的命名实体不变\n",
    "3. 在新生成的句子中，提取与原始“关键词”开头的所有命名实体对应的新实体，并用括号[]括起来\n",
    "例如输入：\n",
    "{{\"template\": \"[问询词0]的[关键词0]是多少？\", \"replace\": {{\"[问询词0]\": \"g20\", \"[关键词0]\": \"是否支持上下水\"}}}},\n",
    "输出：\n",
    "{{\"sentence\": \"请问g20是否有[上下水功能]？\", \"replace\": {{\"是否支持上下水\": \"上下水功能\"}}}}\n",
    "\n",
    "请对每个输入给出三种完全不同的说法（包括句式更改、命名实体转义），最后放在一个列表里面返回, 除此之外不要给出任务其他的信息，即\n",
    "[json0, json1, json2]\n",
    "\n",
    "以下是我的输入：\n",
    "{}\n",
    "输出：\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "46a127af932566b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:49:19.696828430Z",
     "start_time": "2024-04-29T08:49:19.655087163Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=\"https://csagent.openai.azure.com/\",\n",
    "    api_key=\"346ac6661e314a9d8b91b6a99202ba42\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8a3ad74c82cc7445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:49:21.188647167Z",
     "start_time": "2024-04-29T08:49:21.181983065Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_answer(prompt, input_json, model=\"gpt-4-8k\"): # model = \"deployment_name\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model, # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt.format(input_json)},\n",
    "    ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f60eba6868225aa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T10:05:08.321120636Z",
     "start_time": "2024-04-29T08:52:05.011602364Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0010311603546142578\n",
      "10\n",
      "54.72938394546509\n",
      "20\n",
      "74.10766887664795\n",
      "30\n",
      "66.59176898002625\n",
      "40\n",
      "53.349692821502686\n",
      "50\n",
      "72.52705764770508\n",
      "60\n",
      "195.05380153656006\n",
      "70\n",
      "58.798354625701904\n",
      "80\n",
      "66.6578299999237\n",
      "90\n",
      "56.18673753738403\n",
      "100\n",
      "59.12020993232727\n",
      "110\n",
      "68.89034223556519\n",
      "120\n",
      "65.12829995155334\n",
      "130\n",
      "72.94213485717773\n",
      "140\n",
      "67.03654384613037\n",
      "150\n",
      "73.078693151474\n",
      "160\n",
      "61.22945499420166\n",
      "170\n",
      "66.5701413154602\n",
      "180\n",
      "48.48047065734863\n",
      "190\n",
      "54.813912868499756\n",
      "200\n",
      "68.8300507068634\n",
      "210\n",
      "51.10998058319092\n",
      "220\n",
      "63.24893808364868\n",
      "230\n",
      "58.92753338813782\n",
      "240\n",
      "103.40141534805298\n",
      "250\n",
      "92.78640294075012\n",
      "260\n",
      "108.46890020370483\n",
      "270\n",
      "101.64776706695557\n",
      "280\n",
      "98.2324366569519\n",
      "290\n",
      "348.35827112197876\n",
      "300\n",
      "92.04819536209106\n",
      "310\n",
      "98.41790628433228\n",
      "320\n",
      "101.120445728302\n",
      "330\n",
      "85.83840441703796\n",
      "340\n",
      "94.09415578842163\n",
      "350\n",
      "108.66513323783875\n",
      "360\n",
      "99.65352845191956\n",
      "370\n",
      "103.49295353889465\n",
      "380\n",
      "88.19819498062134\n",
      "390\n",
      "96.7720844745636\n",
      "400\n",
      "104.1633551120758\n",
      "410\n",
      "64.83920764923096\n",
      "420\n",
      "79.10374331474304\n",
      "430\n",
      "83.6205108165741\n",
      "440\n",
      "83.25881052017212\n",
      "450\n",
      "83.56475687026978\n",
      "460\n",
      "312.2391746044159\n",
      "470\n",
      "128.60104203224182\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "prev_time = time.time()\n",
    "for i in range(df.shape[0]):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "        cur_time = time.time()\n",
    "        print(cur_time-prev_time)\n",
    "        prev_time = cur_time\n",
    "    item = generate_answer(prompt, df[\"final_prompt\"].iloc[i])\n",
    "    result.append(item)\n",
    "    joblib.dump(item, \"/data/dataset/kefu/gpt4_template/{}.jsonl\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5e5cdf3625ed945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T10:34:02.334714813Z",
     "start_time": "2024-04-29T10:34:02.302279898Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(df.shape[0]):\n",
    "    item = joblib.load(\"/data/dataset/kefu/gpt4_template/{}.jsonl\".format(i))\n",
    "    result.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11f7e95e1bcb4a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T10:50:43.954440541Z",
     "start_time": "2024-04-29T10:50:43.942216149Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_result = []\n",
    "for i in range(len(result)):\n",
    "    try:\n",
    "        item = re.search('：(.*)', result[i], re.DOTALL).group(1)\n",
    "    except:\n",
    "        item = result[i]\n",
    "    \n",
    "    try:\n",
    "        item = json.loads(item)\n",
    "    except:\n",
    "        p = re.compile(r'\\{(.*?)\\}')\n",
    "        matches = p.findall(item)\n",
    "        item = [json.loads('{'+m+'}}') for m in matches]\n",
    "    new_result.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "727c13bd01758905",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T10:53:54.655844648Z",
     "start_time": "2024-04-29T10:53:54.650815469Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"augment\"] = new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2c4e405-4d9c-4f02-8803-cd67f29cf0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'g20标准版的[耗材维护周期]及p10s标准版的[语音助手操作方法]分别是什么？',\n",
       "  'replace': {'耗材更换周期': '耗材维护周期', '如何关闭/开启语音助手': '语音助手操作方法'}},\n",
       " {'sentence': '能否告诉我g20标准版的[耗材更换时间]，以及p10s标准版[如何控制语音助手]？',\n",
       "  'replace': {'耗材更换周期': '耗材更换时间', '如何关闭/开启语音助手': '如何控制语音助手'}},\n",
       " {'sentence': '想了解下g20标准版[换耗材的周期]和p10s标准版[切换语音助手的方法]，你知道吗？',\n",
       "  'replace': {'耗材更换周期': '换耗材的周期', '如何关闭/开启语音助手': '切换语音助手的方法'}}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.augment.iloc[254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff5a682da2c1a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T05:42:05.776482590Z",
     "start_time": "2024-04-30T05:42:05.735320914Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_exploded = df.explode(\"augment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30a6827556022f1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T05:42:06.299762326Z",
     "start_time": "2024-04-30T05:42:06.285857640Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_exploded = df_exploded.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7d40872fa1b98a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T05:42:06.794510400Z",
     "start_time": "2024-04-30T05:42:06.778186750Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_exploded['question'] =  df_exploded['augment'].apply(lambda x: x[\"sentence\"].replace(\"[\", \"\").replace(\"]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ac56769d47fcdae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T05:42:18.694347340Z",
     "start_time": "2024-04-30T05:42:18.678199754Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_indices = []\n",
    "for i in range(df_exploded.shape[0]):\n",
    "    replace = df_exploded['augment'].iloc[i][\"replace\"]\n",
    "    for key in replace:\n",
    "        if key.find(\"[\")>=0:\n",
    "            error_indices.append(i)\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d6463a7-92f0-4908-aabd-0389a3156299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1419, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b39a077891262d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T05:42:19.157183934Z",
     "start_time": "2024-04-30T05:42:19.138461121Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_exploded = df_exploded.drop([df_exploded.index[i] for i in error_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb334afc-a964-4af1-b39b-6a99ff10761b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1412, 7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b727eb06-d182-4b8f-b5b5-5e042e27438c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'g20标准版的商品链接是什么？',\n",
       " 'prompt': [{'primary_value': 'g20标准版',\n",
       "   'key': '商品链接',\n",
       "   '商品链接': 'https://detail.tmall.com/item.htm?id=707235140054&skuId=4969634692760&spm=a21dvs.23580594.0.0.3f063d0d9Hg2Xc'}],\n",
       " 'replace': {'[问询词0]': 'g20标准版', '[关键词0]': '商品链接'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded.gen.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5292cdbccefceda3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T09:07:06.939059736Z",
     "start_time": "2024-04-30T09:07:06.817160181Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954\n",
      "954 type1 error\n"
     ]
    }
   ],
   "source": [
    "ner_list = []\n",
    "for i in range(df_exploded.shape[0]):\n",
    "    replace = df_exploded['augment'].iloc[i][\"replace\"]\n",
    "    primaries = [j['primary_value'] for j in df_exploded['gen'].iloc[i]['prompt']]\n",
    "    flag = False\n",
    "    entity = []\n",
    "    for key in replace:\n",
    "        if replace[key] in primaries:\n",
    "            print(i)\n",
    "            flag = True\n",
    "            break \n",
    "        entity.append(replace[key])\n",
    "    if flag:\n",
    "        print(i, 'type1 error')\n",
    "        ner_list.append(np.nan)\n",
    "        continue\n",
    "    question =  df_exploded['question'].iloc[i]\n",
    "    ner = {}\n",
    "    for item in entity:\n",
    "        matches = list(re.finditer(item, question))\n",
    "        loc = [[j.start(), j.end()-1] for j in matches]\n",
    "        ner[item] = loc \n",
    "    ner_list.append({\"name\": ner})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00f99f6c-19f7-4a71-a1db-567f8a2d3c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat                                                       mopping\n",
       "gen             {'question': 'a10ultra的Wi-Fi连接和a10ultrae的机身尺寸是...\n",
       "template                  [问询词0-0]的[关键词0-0]和[问询词1-0]的[关键词1-0]是什么？\n",
       "type                      gen_different_keywords_different_models\n",
       "final_prompt    {\"template\": \"[问询词0-0]的[关键词0-0]和[问询词1-0]的[关键词1...\n",
       "augment         {'sentence': '请问a10ultra的[WiFi灵敏度]与a10ultrae的[...\n",
       "question                  请问a10ultra的WiFi灵敏度与a10ultrae的设备尺寸各是怎样的？\n",
       "Name: 1000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded.iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e71eefc234129c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T09:07:15.899085653Z",
     "start_time": "2024-04-30T09:07:15.838352467Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': {'WiFi灵敏度': [[11, 17]], '设备尺寸': [[29, 32]]}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_list[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f388b270-1490-4644-b24b-e0e04f76f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"ner_list\"] = ner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d060bc8-eb22-469e-9ac7-0f318c155832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df_exploded[df_exploded[\"ner_list\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d87b082-265b-4752-9271-454f83804277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ner_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>你能提供一下g20标准版的商品访问链接吗？</td>\n",
       "      <td>{'name': {'商品访问链接': [[13, 18]]}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g20标准版的购买链接可以获取吗？</td>\n",
       "      <td>{'name': {'购买链接': [[7, 10]]}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你知道g20标准版在哪里可以找到在线购买页面链接嘛？</td>\n",
       "      <td>{'name': {'在线购买页面链接': [[16, 23]]}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>您能告诉我g20标准版的平台是什么吗？</td>\n",
       "      <td>{'name': {'平台': [[12, 13]]}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>你知道g20标准版所使用的系统平台是什么吗？</td>\n",
       "      <td>{'name': {'系统平台': [[13, 16]]}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     question                            ner_list\n",
       "0       你能提供一下g20标准版的商品访问链接吗？    {'name': {'商品访问链接': [[13, 18]]}}\n",
       "1           g20标准版的购买链接可以获取吗？       {'name': {'购买链接': [[7, 10]]}}\n",
       "2  你知道g20标准版在哪里可以找到在线购买页面链接嘛？  {'name': {'在线购买页面链接': [[16, 23]]}}\n",
       "3         您能告诉我g20标准版的平台是什么吗？        {'name': {'平台': [[12, 13]]}}\n",
       "4      你知道g20标准版所使用的系统平台是什么吗？      {'name': {'系统平台': [[13, 16]]}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_exploded[[\"question\", \"ner_list\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "566a68c0-550b-4f78-a418-db291dc96586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"bio\"] = df_exploded[[\"question\", \"ner_list\"]].apply(lambda x: bio_tagging(x[\"question\"], x[\"ner_list\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c670262-1040-462c-b728-8c756f086ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"bio_words\"] = df_exploded[\"bio\"].apply(lambda x: x[0])\n",
    "df_exploded[\"bio_tags\"] = df_exploded[\"bio\"].apply(lambda x: x[1])\n",
    "df_exploded = df_exploded.drop(\"bio\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1414e789-5347-4f18-a57f-a493dededcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {'O': 0, 'B-name': 1, 'I-name': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3326c591-348e-4d84-864d-919e03e57438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(x, tag2id):\n",
    "    new_list = []\n",
    "    for i in x:\n",
    "        new_list.append(tag2id[i])\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4453aa7d-d17e-45f1-8518-8dba00ec372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"bio_tags_id\"] = df_exploded[\"bio_tags\"].apply(lambda x:tagging(x, tag2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1fe9df02-9fb9-44fb-8f67-f1b8a12dda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "699e11c4-6628-4cb7-8c14-51dcc20cf064",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_set = list(df_all.columns)\n",
    "exclude_cols = [\"平台ID\", \"商品id\", \"商品编码\", \"商品分类\", \"商品名字\", \"版本\", \"商品型号\", \"primary_key\"]\n",
    "schema_set = [col for col in schema_set if col not in exclude_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15119e4e-bf3f-49aa-a3e8-80fa0d261768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = HuggingFaceBgeEmbeddings(model_name=\"/data/dataset/huggingface/hub/bge-large-zh-v1.5\", \n",
    "#                                    encode_kwargs = {'normalize_embeddings': True},\n",
    "#                                    model_kwargs={'device': 'cuda:0'})\n",
    "encoder = HuggingFaceEmbeddings(model_name=\"/data/dataset/huggingface/hub/bge-large-zh-v1.5\", \n",
    "                                   encode_kwargs = {'normalize_embeddings': True},\n",
    "                                   model_kwargs={'device': 'cuda:0'})\n",
    "store = FAISS.from_documents([Document(page_content=line, metadata={\"id\": id})\n",
    " for id, line in enumerate(schema_set)], embedding=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ff0e7-52bf-4ada-ba00-591beb283b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "schema_set = list(df_all.columns)\n",
    "exclude_cols = [\"平台ID\", \"商品id\", \"商品编码\", \"商品分类\", \"商品名字\", \"版本\", \"商品型号\", \"primary_key\"]\n",
    "schema_set = [col for col in schema_set if col not in exclude_cols]\n",
    "\n",
    "# encoder = HuggingFaceBgeEmbeddings(model_name=\"/data/dataset/huggingface/hub/bge-large-zh-v1.5\", \n",
    "#                                    encode_kwargs = {'normalize_embeddings': True},\n",
    "#                                    model_kwargs={'device': 'cuda:0'})\n",
    "encoder = HuggingFaceEmbeddings(model_name=\"/data/dataset/huggingface/hub/bge-large-zh-v1.5\", \n",
    "                                   encode_kwargs = {'normalize_embeddings': True},\n",
    "                                   model_kwargs={'device': 'cuda:0'})\n",
    "store = FAISS.from_documents([Document(page_content=line, metadata={\"id\": id})\n",
    " for id, line in enumerate(schema_set)], embedding=encoder)\n",
    "\n",
    "from fastbm25 import fastbm25\n",
    "from utils import WordCut\n",
    "wc = WordCut()\n",
    "\n",
    "bm25 = fastbm25([wc.cut(doc) for doc in schema_set])\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "def recall_from_edit_distance(word, store, schema_set, recall_n=3):\n",
    "    searched = store.similarity_search_with_score(word, k=recall_n)\n",
    "    recalls = [(schema_set[i[0].metadata[\"id\"]], i[1]) for i in searched]\n",
    "    # candidates = [i[0] for i in recalls]\n",
    "    # edit_lens = [edit_distance(i, word) for i in candidates]\n",
    "    # return [cand for i, cand in enumerate(candidates) if edit_lens[i] != max(len(candidates[i]), len(word)) and edit_lens[i] <= 4]\n",
    "    results = sorted(recalls, key=lambda x: x[1])[:recall_n]\n",
    "    results = [i[0] for i in results]\n",
    "    return results \n",
    "\n",
    "def recall_from_edit_distance2(word, store, schema_set, recall_n=3):\n",
    "    searched = bm25.top_k_sentence(wc.cut(word.lower()), k=recall_n)\n",
    "    results = [schema_set[i[1]] for i in searched]\n",
    "    return results \n",
    "\n",
    "def prepare_columns_for_sql_v2(query_body, store, schema_set):\n",
    "    query = query_body[\"query\"]\n",
    "    all_cols = []\n",
    "\n",
    "    keywords = [query[entity[\"start\"]:entity[\"end\"]] for entity in query_body.get(\"entities\", [])]\n",
    "    for keyword in keywords:\n",
    "        if keyword in schema_set:\n",
    "            all_cols.append(keyword)\n",
    "\n",
    "    aa = [query]\n",
    "    for keyword in aa:\n",
    "        all_cols += recall_from_edit_distance2(keyword, store, schema_set)\n",
    "\n",
    "    all_cols_set = []\n",
    "    for col in all_cols:\n",
    "        if col not in all_cols_set:\n",
    "            all_cols_set.append(col)\n",
    "\n",
    "    return all_cols_set\n",
    "\n",
    "def get_keywords(model, query):\n",
    "    entities = model(query)\n",
    "    return {\"query\": query, \"entities\": entities}\n",
    "\n",
    "from transformers import pipeline\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"/workspace/data/private/zhuxiaohai/models/bert_finetuned_ner_chinese/\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "df_exploded[\"ner\"] = df_exploded[\"question\"].apply(lambda x: get_keywords(token_classifier, x))\n",
    "df_exploded[\"pred\"] = df_exploded[\"ner\"].apply(lambda x: prepare_columns_for_sql_v2(x, bm25, schema_set))\n",
    "\n",
    "def metric(pred, gt):\n",
    "    gt_key = []\n",
    "    for item in gt:\n",
    "        for key in item[\"key\"].split(\"|\"):\n",
    "            gt_key.append(key)\n",
    "    gt_key = set(gt_key)\n",
    "    pred = set(pred)\n",
    "    match = gt_key & pred\n",
    "    if len(pred) > 0:\n",
    "        precision = len(match)/ len(pred)\n",
    "    else:\n",
    "        if len(gt_key) == 0:\n",
    "            precision = 1\n",
    "        else:\n",
    "            precision = 0\n",
    "    if len(gt_key) > 0:\n",
    "        recall = len(match) / len(gt_key)\n",
    "    else:\n",
    "        recall = 1\n",
    "    return precision, recall \n",
    "\n",
    "df_exploded[\"metric\"] = df_exploded[[\"pred\", \"gen\"]].apply(lambda x: metric(x[\"pred\"], x[\"gen\"][\"prompt\"]), axis=1)\n",
    "df_exploded[\"precision\"] = df_exploded[\"metric\"].apply(lambda x: x[0])\n",
    "df_exploded[\"recall\"] = df_exploded[\"metric\"].apply(lambda x: x[1])\n",
    "\n",
    "df_exploded[\"precision\"].mean()\n",
    "df_exploded[\"recall\"].mean()\n",
    "df_exploded[\"recall\"].value_counts(\"mean\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "请你给我写一个sql从数据库查询型号参数，必须在查询结果中包括所有涉及的型号， 但是你需要根据我的问题从涉及的字段中准确选择而不是简单使用全部字段或者捏造字段。直接给出sql除此之外不要返回任何东西。\n",
    "1. 表名：df\n",
    "2. 涉及的型号：{}\n",
    "3. 涉及的字段：{}\n",
    "\n",
    "我的问题是：\n",
    "{}\n",
    "sql:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_answer3(prompt, model_list, col_list, question, model=\"gpt-4-8k\"): # model = \"deployment_name\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model, # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt.format(\",\".join(model_list), \",\".join(col_list), question)},\n",
    "    ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "df_exploded[\"model_list\"] = df_exploded['gen'].apply(lambda x: [item[\"primary_value\"] for item in x[\"prompt\"]])\n",
    "\n",
    "result = []\n",
    "prev_time = time.time()\n",
    "for i in range(df_exploded.shape[0]):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "        cur_time = time.time()\n",
    "        print(cur_time-prev_time)\n",
    "        prev_time = cur_time\n",
    "    item = generate_answer3(prompt, df_exploded.iloc[i].model_list, df_exploded.iloc[i].pred, df_exploded.iloc[i].question)\n",
    "    result.append(item)\n",
    "    joblib.dump(item, \"/data/dataset/kefu/gpt4_template_sql/{}.jsonl\".format(i))\n",
    "\n",
    "result = []\n",
    "for i in range(df_exploded.shape[0]):\n",
    "    item = joblib.load(\"/data/dataset/kefu/gpt4_template_sql/{}.jsonl\".format(i))\n",
    "    result.append(item)\n",
    "\n",
    "df_exploded[\"sql\"] = result\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_field(field):\n",
    "    return field.strip().strip('`').strip('\"').strip(\"'\")\n",
    "\n",
    "def extract_fields_and_models(sql_query):\n",
    "    # 提取所有 SELECT 和 FROM 之间的部分\n",
    "    select_from_pattern = r\"SELECT\\s+(.*?)\\s+FROM\"\n",
    "    select_from_matches = re.findall(select_from_pattern, sql_query, re.DOTALL)\n",
    "    select_from_fields = [clean_field(field) for match in select_from_matches for field in match.split(',')]\n",
    "\n",
    "    # 提取所有 WHERE 型号 IN 或 WHERE 型号 = 后面的部分\n",
    "    where_in_or_equal_pattern = r\"WHERE\\s+`?型号`?\\s*=\\s*['\\\"]([^'\\\"]*)['\\\"]|WHERE\\s+`?型号`?\\s*IN\\s*\\(([^)]*)\\)\"\n",
    "    where_in_or_equal_matches = re.findall(where_in_or_equal_pattern, sql_query, re.DOTALL)\n",
    "    \n",
    "    where_fields = []\n",
    "    for match in where_in_or_equal_matches:\n",
    "        if match[0]:\n",
    "            where_fields.append(clean_field(match[0]))\n",
    "        elif match[1]:\n",
    "            where_fields.extend([clean_field(model) for model in match[1].split(',')])\n",
    "\n",
    "    return select_from_fields, where_fields\n",
    "\n",
    "def process_union_queries(sql_query):\n",
    "    # 分割 UNION ALL 语句\n",
    "    union_queries = sql_query.split('UNION ALL')\n",
    "    \n",
    "    all_fields = []\n",
    "    all_models = []\n",
    "    \n",
    "    for query in union_queries:\n",
    "        fields, models = extract_fields_and_models(query)\n",
    "        all_fields.extend(fields)\n",
    "        all_models.extend(models)\n",
    "    \n",
    "    return all_fields, all_models\n",
    "\n",
    "df_exploded[\"sql_extracted\"] = df_exploded[\"sql\"].apply(lambda x: process_union_queries(x)[0])\n",
    "df_exploded[\"sql_metric\"] = df_exploded[[\"sql_extracted\", \"gen\"]].apply(\n",
    "    lambda x: metric(x[\"sql_extracted\"], x[\"gen\"][\"prompt\"]), axis=1)\n",
    "df_exploded[\"sql_precision\"] = df_exploded[\"sql_metric\"].apply(lambda x: x[0])\n",
    "df_exploded[\"sql_recall\"] = df_exploded[\"sql_metric\"].apply(lambda x: x[1])\n",
    "df_exploded[\"recall\"].value_counts(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "476f4991-0baf-4ddb-b48d-02a05923486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbm25 import fastbm25\n",
    "from utils import WordCut\n",
    "wc = WordCut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "23bcb1ee-8e9b-4a07-817a-332f7d0a3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = fastbm25([wc.cut(doc) for doc in schema_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68e1a592-9dc1-42b5-ad2e-ce910da88dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "76c7635c-ef3d-46c1-b0a4-b07eff89f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_from_edit_distance(word, store, schema_set, recall_n=3):\n",
    "    searched = store.similarity_search_with_score(word, k=recall_n)\n",
    "    recalls = [(schema_set[i[0].metadata[\"id\"]], i[1]) for i in searched]\n",
    "    # candidates = [i[0] for i in recalls]\n",
    "    # edit_lens = [edit_distance(i, word) for i in candidates]\n",
    "    # return [cand for i, cand in enumerate(candidates) if edit_lens[i] != max(len(candidates[i]), len(word)) and edit_lens[i] <= 4]\n",
    "    results = sorted(recalls, key=lambda x: x[1])[:recall_n]\n",
    "    results = [i[0] for i in results]\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3540b1e5-dab5-4252-9063-d81b451073c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_from_edit_distance2(word, store, schema_set, recall_n=3):\n",
    "    searched = bm25.top_k_sentence(wc.cut(word.lower()), k=recall_n)\n",
    "    results = [schema_set[i[1]] for i in searched]\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "74e767c8-52a0-4376-9900-08a1c6ed5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_columns_for_sql_v2(query_body, store, schema_set):\n",
    "    query = query_body[\"query\"]\n",
    "    all_cols = []\n",
    "\n",
    "    keywords = [query[entity[\"start\"]:entity[\"end\"]] for entity in query_body.get(\"entities\", [])]\n",
    "    for keyword in keywords:\n",
    "        if keyword in schema_set:\n",
    "            all_cols.append(keyword)\n",
    "\n",
    "    aa = [query]\n",
    "    for keyword in aa:\n",
    "        all_cols += recall_from_edit_distance2(keyword, store, schema_set)\n",
    "\n",
    "    all_cols_set = []\n",
    "    for col in all_cols:\n",
    "        if col not in all_cols_set:\n",
    "            all_cols_set.append(col)\n",
    "\n",
    "    return all_cols_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f0235343-7249-4852-ba0b-f2127d64ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(model, query):\n",
    "    entities = model(query)\n",
    "    return {\"query\": query, \"entities\": entities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51acaf9b-ef8c-4cfe-a6d7-5cb9e582d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"/workspace/data/private/zhuxiaohai/models/bert_finetuned_ner_chinese/\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2bdb7e40-0397-4bcd-896e-28ad0524db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"ner\"] = df_exploded[\"question\"].apply(lambda x: get_keywords(token_classifier, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a257d641-540a-4650-838c-8dd0396d99eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_exploded[\"pred\"] = df_exploded[\"ner\"].apply(lambda x: prepare_columns_for_sql_v2(x, bm25, schema_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d8f22e3c-d6e3-4a95-ab64-cd7abbdfc1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(pred, gt):\n",
    "    gt_key = []\n",
    "    for item in gt:\n",
    "        for key in item[\"key\"].split(\"|\"):\n",
    "            gt_key.append(key)\n",
    "    gt_key = set(gt_key)\n",
    "    pred = set(pred)\n",
    "    match = gt_key & pred\n",
    "    if len(pred) > 0:\n",
    "        precision = len(match)/ len(pred)\n",
    "    else:\n",
    "        if len(gt_key) == 0:\n",
    "            precision = 1\n",
    "        else:\n",
    "            precision = 0\n",
    "    if len(gt_key) > 0:\n",
    "        recall = len(match) / len(gt_key)\n",
    "    else:\n",
    "        recall = 1\n",
    "    return precision, recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fa6b7f02-2dd1-499b-b724-4097d40a7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"metric\"] = df_exploded[[\"pred\", \"gen\"]].apply(lambda x: metric(x[\"pred\"], x[\"gen\"][\"prompt\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6bf6f824-bc33-445c-9d56-45a3ced5c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"precision\"] = df_exploded[\"metric\"].apply(lambda x: x[0])\n",
    "df_exploded[\"recall\"] = df_exploded[\"metric\"].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c81255d2-8636-417d-b1c3-82a8c89fb4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37957713205764226"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded[\"precision\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9943c8ea-e94a-4320-9b6e-4c84fe32246c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6293408929836996"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded[\"recall\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "343a6762-dd68-48fc-a2d8-b028d243bce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recall\n",
       "1.000000    0.516655\n",
       "0.000000    0.257264\n",
       "0.500000    0.218285\n",
       "0.333333    0.002835\n",
       "0.250000    0.002126\n",
       "0.666667    0.001417\n",
       "0.833333    0.000709\n",
       "0.750000    0.000709\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded[\"recall\"].value_counts(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff3ceb11-f705-4938-a029-272537d3b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "请你给我写一个sql从数据库查询型号参数，必须在查询结果中包括所有涉及的型号， 但是你需要根据我的问题从涉及的字段中准确选择而不是简单使用全部字段或者捏造字段。直接给出sql除此之外不要返回任何东西。\n",
    "1. 表名：df\n",
    "2. 涉及的型号：{}\n",
    "3. 涉及的字段：{}\n",
    "\n",
    "我的问题是：\n",
    "{}\n",
    "sql:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "faac0550-3799-4567-b281-a69a613603f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer3(prompt, model_list, col_list, question, model=\"gpt-4-8k\"): # model = \"deployment_name\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model, # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt.format(\",\".join(model_list), \",\".join(col_list), question)},\n",
    "    ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d85a9b7-7ea2-4d2c-b6c2-5d57057c7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"model_list\"] = df_exploded['gen'].apply(lambda x: [item[\"primary_value\"] for item in x[\"prompt\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "28ba18a5-db41-4b15-93cb-633641481dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0002491474151611328\n",
      "10\n",
      "25.573196411132812\n",
      "20\n",
      "27.979446411132812\n",
      "30\n",
      "27.555006980895996\n",
      "40\n",
      "34.710694551467896\n",
      "50\n",
      "24.477225303649902\n",
      "60\n",
      "26.7535879611969\n",
      "70\n",
      "28.095032215118408\n",
      "80\n",
      "27.817243099212646\n",
      "90\n",
      "27.5528302192688\n",
      "100\n",
      "26.708733558654785\n",
      "110\n",
      "23.29882001876831\n",
      "120\n",
      "25.595393180847168\n",
      "130\n",
      "27.23554825782776\n",
      "140\n",
      "29.92163896560669\n",
      "150\n",
      "25.806005001068115\n",
      "160\n",
      "25.61414408683777\n",
      "170\n",
      "22.355963706970215\n",
      "180\n",
      "24.050411462783813\n",
      "190\n",
      "28.27499508857727\n",
      "200\n",
      "28.75628638267517\n",
      "210\n",
      "23.84361243247986\n",
      "220\n",
      "25.905872583389282\n",
      "230\n",
      "27.91088581085205\n",
      "240\n",
      "28.230652332305908\n",
      "250\n",
      "29.111735582351685\n",
      "260\n",
      "20.064685583114624\n",
      "270\n",
      "25.15457057952881\n",
      "280\n",
      "26.486813068389893\n",
      "290\n",
      "24.032265424728394\n",
      "300\n",
      "31.94511604309082\n",
      "310\n",
      "23.106252670288086\n",
      "320\n",
      "26.384773015975952\n",
      "330\n",
      "35.598450660705566\n",
      "340\n",
      "30.126078128814697\n",
      "350\n",
      "29.348214626312256\n",
      "360\n",
      "25.54759407043457\n",
      "370\n",
      "25.56773567199707\n",
      "380\n",
      "27.044626235961914\n",
      "390\n",
      "25.328826427459717\n",
      "400\n",
      "29.640239000320435\n",
      "410\n",
      "34.913729429244995\n",
      "420\n",
      "24.301784992218018\n",
      "430\n",
      "24.54772400856018\n",
      "440\n",
      "28.756462812423706\n",
      "450\n",
      "28.24626636505127\n",
      "460\n",
      "27.93321394920349\n",
      "470\n",
      "30.70799946784973\n",
      "480\n",
      "28.219082593917847\n",
      "490\n",
      "30.931368589401245\n",
      "500\n",
      "22.267035961151123\n",
      "510\n",
      "22.200114965438843\n",
      "520\n",
      "24.244516134262085\n",
      "530\n",
      "20.39182996749878\n",
      "540\n",
      "23.90523338317871\n",
      "550\n",
      "21.884539365768433\n",
      "560\n",
      "21.729112148284912\n",
      "570\n",
      "29.998209953308105\n",
      "580\n",
      "24.295003652572632\n",
      "590\n",
      "23.32473659515381\n",
      "600\n",
      "23.35634469985962\n",
      "610\n",
      "21.901610851287842\n",
      "620\n",
      "21.18933629989624\n",
      "630\n",
      "24.113226890563965\n",
      "640\n",
      "24.426483869552612\n",
      "650\n",
      "34.66841745376587\n",
      "660\n",
      "23.831104278564453\n",
      "670\n",
      "29.103309631347656\n",
      "680\n",
      "21.633225917816162\n",
      "690\n",
      "33.45242953300476\n",
      "700\n",
      "37.98841404914856\n",
      "710\n",
      "40.46210312843323\n",
      "720\n",
      "48.64774036407471\n",
      "730\n",
      "41.57205057144165\n",
      "740\n",
      "51.94901156425476\n",
      "750\n",
      "73.21640539169312\n",
      "760\n",
      "52.73073625564575\n",
      "770\n",
      "49.29559564590454\n",
      "780\n",
      "41.803202390670776\n",
      "790\n",
      "55.6915123462677\n",
      "800\n",
      "41.894548416137695\n",
      "810\n",
      "44.48377442359924\n",
      "820\n",
      "45.74658131599426\n",
      "830\n",
      "37.253013610839844\n",
      "840\n",
      "45.875059366226196\n",
      "850\n",
      "42.601919412612915\n",
      "860\n",
      "42.16761326789856\n",
      "870\n",
      "39.40120244026184\n",
      "880\n",
      "39.87957406044006\n",
      "890\n",
      "40.08110022544861\n",
      "900\n",
      "42.14566135406494\n",
      "910\n",
      "34.12943410873413\n",
      "920\n",
      "32.85134983062744\n",
      "930\n",
      "39.43753910064697\n",
      "940\n",
      "40.902451038360596\n",
      "950\n",
      "44.352147579193115\n",
      "960\n",
      "42.92235088348389\n",
      "970\n",
      "38.87868905067444\n",
      "980\n",
      "33.845271587371826\n",
      "990\n",
      "39.75371026992798\n",
      "1000\n",
      "47.129756689071655\n",
      "1010\n",
      "43.84941291809082\n",
      "1020\n",
      "48.05512809753418\n",
      "1030\n",
      "51.23466944694519\n",
      "1040\n",
      "35.948275566101074\n",
      "1050\n",
      "37.21337127685547\n",
      "1060\n",
      "33.198970079422\n",
      "1070\n",
      "45.5502347946167\n",
      "1080\n",
      "36.16295886039734\n",
      "1090\n",
      "32.00709581375122\n",
      "1100\n",
      "32.93310213088989\n",
      "1110\n",
      "36.46404433250427\n",
      "1120\n",
      "36.96360111236572\n",
      "1130\n",
      "32.85421800613403\n",
      "1140\n",
      "34.52692747116089\n",
      "1150\n",
      "31.12155771255493\n",
      "1160\n",
      "33.976951122283936\n",
      "1170\n",
      "42.78260016441345\n",
      "1180\n",
      "38.97910165786743\n",
      "1190\n",
      "50.32024431228638\n",
      "1200\n",
      "29.7238130569458\n",
      "1210\n",
      "27.27943468093872\n",
      "1220\n",
      "31.445510864257812\n",
      "1230\n",
      "26.40797448158264\n",
      "1240\n",
      "27.965299367904663\n",
      "1250\n",
      "33.02440643310547\n",
      "1260\n",
      "37.770113706588745\n",
      "1270\n",
      "40.0203754901886\n",
      "1280\n",
      "27.238288402557373\n",
      "1290\n",
      "28.956084489822388\n",
      "1300\n",
      "31.704891443252563\n",
      "1310\n",
      "34.20486831665039\n",
      "1320\n",
      "32.214439868927\n",
      "1330\n",
      "34.01336646080017\n",
      "1340\n",
      "32.00744438171387\n",
      "1350\n",
      "29.088072538375854\n",
      "1360\n",
      "38.92903995513916\n",
      "1370\n",
      "35.09884715080261\n",
      "1380\n",
      "41.41639065742493\n",
      "1390\n",
      "47.26696968078613\n",
      "1400\n",
      "58.71310758590698\n",
      "1410\n",
      "69.43316984176636\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "prev_time = time.time()\n",
    "for i in range(df_exploded.shape[0]):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "        cur_time = time.time()\n",
    "        print(cur_time-prev_time)\n",
    "        prev_time = cur_time\n",
    "    item = generate_answer3(prompt, df_exploded.iloc[i].model_list, df_exploded.iloc[i].pred, df_exploded.iloc[i].question)\n",
    "    result.append(item)\n",
    "    joblib.dump(item, \"/data/dataset/kefu/gpt4_template_sql/{}.jsonl\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae56ecd2-02fb-4a07-8082-00baa6350ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(df_exploded.shape[0]):\n",
    "    item = joblib.load(\"/data/dataset/kefu/gpt4_template_sql/{}.jsonl\".format(i))\n",
    "    result.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "254b990d-dc20-4a19-868f-df02140fc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"sql\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "758e9aa9-beee-413d-940b-1612064a44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_field(field):\n",
    "    return field.strip().strip('`').strip('\"').strip(\"'\")\n",
    "\n",
    "def extract_fields_and_models(sql_query):\n",
    "    # 提取所有 SELECT 和 FROM 之间的部分\n",
    "    select_from_pattern = r\"SELECT\\s+(.*?)\\s+FROM\"\n",
    "    select_from_matches = re.findall(select_from_pattern, sql_query, re.DOTALL)\n",
    "    select_from_fields = [clean_field(field) for match in select_from_matches for field in match.split(',')]\n",
    "\n",
    "    # 提取所有 WHERE 型号 IN 或 WHERE 型号 = 后面的部分\n",
    "    where_in_or_equal_pattern = r\"WHERE\\s+`?型号`?\\s*=\\s*['\\\"]([^'\\\"]*)['\\\"]|WHERE\\s+`?型号`?\\s*IN\\s*\\(([^)]*)\\)\"\n",
    "    where_in_or_equal_matches = re.findall(where_in_or_equal_pattern, sql_query, re.DOTALL)\n",
    "    \n",
    "    where_fields = []\n",
    "    for match in where_in_or_equal_matches:\n",
    "        if match[0]:\n",
    "            where_fields.append(clean_field(match[0]))\n",
    "        elif match[1]:\n",
    "            where_fields.extend([clean_field(model) for model in match[1].split(',')])\n",
    "\n",
    "    return select_from_fields, where_fields\n",
    "\n",
    "def process_union_queries(sql_query):\n",
    "    # 分割 UNION ALL 语句\n",
    "    union_queries = sql_query.split('UNION ALL')\n",
    "    \n",
    "    all_fields = []\n",
    "    all_models = []\n",
    "    \n",
    "    for query in union_queries:\n",
    "        fields, models = extract_fields_and_models(query)\n",
    "        all_fields.extend(fields)\n",
    "        all_models.extend(models)\n",
    "    \n",
    "    return all_fields, all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e6cd91b-b8c4-40d3-af7c-6c8b70965273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"sql_extracted\"] = df_exploded[\"sql\"].apply(lambda x: process_union_queries(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9dcbf660-26e3-4408-9236-be91a3231391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"sql_metric\"] = df_exploded[[\"sql_extracted\", \"gen\"]].apply(\n",
    "    lambda x: metric(x[\"sql_extracted\"], x[\"gen\"][\"prompt\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d78163dd-52be-4fa2-9e58-111be835e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded[\"sql_precision\"] = df_exploded[\"sql_metric\"].apply(lambda x: x[0])\n",
    "df_exploded[\"sql_recall\"] = df_exploded[\"sql_metric\"].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a114adc-37d4-4ea3-871c-74782e0cfaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recall\n",
       "1.000000    0.745571\n",
       "0.500000    0.197732\n",
       "0.000000    0.048193\n",
       "0.666667    0.002835\n",
       "0.333333    0.002126\n",
       "0.250000    0.001417\n",
       "0.166667    0.001417\n",
       "0.833333    0.000709\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded[\"recall\"].value_counts(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "864ad50a-c589-461a-8f22-466637065007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-name', 'I-name'], id=None), length=-1, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features = Features({'ner_tags': Sequence(ClassLabel(num_classes=3, names=['O', 'B-name', 'I-name'])),\n",
    "                     'tokens': Sequence(Value(dtype='string'))})\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78130d43-ab19-47a2-9dda-965653458ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = Dataset.from_pandas(df_exploded[[\"bio_words\", \"bio_tags_id\"]].rename(columns={\"bio_words\": \"tokens\", \"bio_tags_id\": \"ner_tags\"}),\n",
    "                    features=features, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "855a5dad-8045-44a2-a85e-d5677b624324",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.train_test_split(test_size=0.2, seed=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "212cba71-a141-4e77-8560-16776ee02731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7087a48c9233440ab7f12d079e8dd5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5eeff1f6174b72a03fb2db88b70e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets.save_to_disk(\"/data/dataset/kefu/ner_from_template_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f8ca6-0265-40be-8a09-daac7f395c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对用户提问打标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37373e5d-e0fa-4788-9427-6464b783366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = [\"平台ID\", \"商品id\", \"商品编码\", \"商品分类\", \"商品名字\", \"版本\", \"商品型号\", \"primary_key\"]\n",
    "candidate_cols = []\n",
    "for col in df_all.columns:\n",
    "    if col not in exclude_cols:\n",
    "        candidate_cols.append(col)\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "我要做命名实体识别模型，请你在输入的句子中提取实体，要提取的实体必须与以下家庭清洁机器人的常用参数有关，常用参数列表如下：\n",
    "{}\n",
    "\n",
    "请将提取的实体和它对应的常用参数以键值对的形式给出，后者必须从常用参数列表里面选择，不得捏造，如果输入句子中没有与常用参数列表中所列的参数相关的实体就不要提取任何东西。最后把所有结果放在一个列表里面返回，除此之外不要输出其他任何信息，举例如下：\n",
    "输入句子：\n",
    "P20标准版的额定输入(电压)是多少？\n",
    "输出：\n",
    "[{{\"额定输入(电压)\": \"额定输入(电压)\"}}]\n",
    "\n",
    "输入句子：\n",
    "我想了解一下a10ultra的商品访问链接及a10ultrae换耗材的周期？\n",
    "输出：\n",
    "[{{\"商品访问链接\": \"商品链接\"}}, {{\"换耗材的周期\": \"耗材更换周期\"}}]\n",
    "\n",
    "输入句子：\n",
    "A10报错？\n",
    "输出：\n",
    "[]\n",
    "\n",
    "输入句子：\n",
    "{}\n",
    "输出：\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dc9d0-ebf8-495a-b18b-f9d42597b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer2(prompt, candidate_cols, input_json, model=\"gpt-4-8k\"): # model = \"deployment_name\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model, # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt.format(json.dumps(candidate_cols, ensure_ascii=False), input_json)},\n",
    "    ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "result = []\n",
    "prev_time = time.time()\n",
    "for i in range(oot.shape[0]):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "        cur_time = time.time()\n",
    "        print(cur_time-prev_time)\n",
    "        prev_time = cur_time\n",
    "    item = generate_answer2(prompt2, oot[\"question\"].iloc[i])\n",
    "    result.append(item)\n",
    "    joblib.dump(item, \"/data/dataset/kefu/gpt4_ner/{}.jsonl\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e26e477-562a-47fa-b706-1b67d962cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(oot.shape[0]):\n",
    "    item = joblib.load(\"/data/dataset/kefu/gpt4_ner/{}.jsonl\".format(i))\n",
    "    result.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5107355c-375a-4267-b160-8ad20aee0226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437\n",
      "4158\n"
     ]
    }
   ],
   "source": [
    "new_result = []\n",
    "for i in range(len(result)):\n",
    "    try:\n",
    "        item = json.loads(result[i])\n",
    "    except:\n",
    "        print(i)\n",
    "        new = result[i][0]+\"{\"+result[i][1:-1]+\"}\"+result[i][-1]\n",
    "        item = json.loads(new)\n",
    "    new_result.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e578ea1-2ca3-4e0f-98dd-04d23096d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oot[\"ner\"] = new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "81052990-5cde-4831-b4e4-0ca2fbfb4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_list = []\n",
    "ner_number_list = []\n",
    "for i in range(oot.shape[0]):\n",
    "    question = oot.question.iloc[i]\n",
    "    entity = []\n",
    "    replace = oot['ner'].iloc[i]\n",
    "    for item in replace:\n",
    "        for key in item.keys():\n",
    "            if key not in entity:\n",
    "                entity.append(key)\n",
    "    ner = {}\n",
    "    ner_number = 0\n",
    "    for item in entity:\n",
    "        if item.lower() in all_model_list:\n",
    "            continue\n",
    "        matches = list(re.finditer(item, question))\n",
    "        loc = [[j.start(), j.end()-1] for j in matches]\n",
    "        if len(loc) > 0:\n",
    "            ner[item] = loc \n",
    "        ner_number += len(loc)\n",
    "    ner_list.append({\"name\": ner})\n",
    "    ner_number_list.append(ner_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fafe396e-37c8-4136-8d57-7e5af2a7d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "oot[\"ner_list\"] = ner_list\n",
    "oot[\"ner_num\"] = ner_number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6a318695-833b-4995-8cd3-a7af174ea51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oot[\"bio\"] = oot[[\"question\", \"ner_list\"]].apply(lambda x: bio_tagging(x[\"question\"], x[\"ner_list\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "a5e2510b-8439-4be6-bde1-409ee06c58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oot[\"bio_words\"] = oot[\"bio\"].apply(lambda x: x[0])\n",
    "oot[\"bio_tags\"] = oot[\"bio\"].apply(lambda x: x[1])\n",
    "oot = oot.drop(\"bio\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "2e85a44e-53d9-4747-b54b-371856e3cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {'O': 0, 'B-name': 1, 'I-name': 2}\n",
    "def tagging(x, tag2id):\n",
    "    new_list = []\n",
    "    for i in x:\n",
    "        new_list.append(tag2id[i])\n",
    "    return new_list\n",
    "oot[\"bio_tags_id\"] = oot[\"bio_tags\"].apply(lambda x:tagging(x, tag2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "91fb634a-a0e9-462a-a638-e4227b9beb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({'ner_tags': Sequence(ClassLabel(num_classes=3, names=['O', 'B-name', 'I-name'])),\n",
    "                     'tokens': Sequence(Value(dtype='string')),\n",
    "                     \"ner_num\": ClassLabel(num_classes=4, names=[0,1,2,3]),\n",
    "                     \"index\": Value(dtype='string')})\n",
    "raw_datasets = Dataset.from_pandas(oot[[\"bio_words\", \"bio_tags_id\", \"ner_num\"]\n",
    "                                   ].reset_index().astype({\"index\": str}).rename(\n",
    "                                       columns={\"bio_words\": \"tokens\", \"bio_tags_id\": \"ner_tags\"}),\n",
    "                    features=features, preserve_index=False)\n",
    "raw_datasets = raw_datasets.train_test_split(test_size=0.2, seed=42, shuffle=True, stratify_by_column=\"ner_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3b991965-fa4f-45a4-b5c7-08103e987559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4c5740d5e345d28036d9e4bf8d1bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6747799b7440288e0a9ae62de78384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1070 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets.save_to_disk(\"/data/dataset/kefu/ner_from_cs_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c189a9c3-0ac4-47ff-958e-3ae8dd3e4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做NER模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "589d7420-f0ba-4e1d-9f6c-6dacf259a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_from_disk(\"/data/dataset/kefu/ner_from_cs_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "cede4933-0a0b-469b-9ad5-382de8c4fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mambaforge/envs/python310_chatbot/lib/python3.10/site-packages/datasets/load.py:926: FutureWarning: The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /data/dataset/huggingface/hub/conll2003/conll2003.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"/data/dataset/huggingface/hub/conll2003/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "219c5b66-83bd-4db3-b0ce-759493aa114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_from_disk(\"/data/dataset/kefu/ner_from_template_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "c08a8012-ba6e-4a96-ac99-c677ec312f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-name', 'I-name']"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets['train'].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "040c0a2e-6b75-440f-a92f-493bf7713764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g 2 0 标 准 版 的 进      水      压      力      最      大      值      以 及 p 1 0 s 标 准 版 的 洗      衣      热      水      功      能      分 别 是 什 么 ？ \n",
      "O O O O O O O B-name I-name I-name I-name I-name I-name I-name O O O O O O O O O O B-name I-name I-name I-name I-name I-name O O O O O O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets['train'][0][\"tokens\"]\n",
    "labels = raw_datasets['train'][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "dcd051e5-bfde-480d-887b-a64a67c86738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/dataset/huggingface/hub/bert-base-chinese')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/data/dataset/huggingface/hub/bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "2df79f6a-1c41-46bd-beca-a615ea983b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "cd16ed44-4fa9-48c4-b142-79f3b31d84e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'g',\n",
       " '2',\n",
       " '0',\n",
       " '标',\n",
       " '准',\n",
       " '版',\n",
       " '的',\n",
       " '进',\n",
       " '水',\n",
       " '压',\n",
       " '力',\n",
       " '最',\n",
       " '大',\n",
       " '值',\n",
       " '以',\n",
       " '及',\n",
       " 'p',\n",
       " '1',\n",
       " '0',\n",
       " 's',\n",
       " '标',\n",
       " '准',\n",
       " '版',\n",
       " '的',\n",
       " '洗',\n",
       " '衣',\n",
       " '热',\n",
       " '水',\n",
       " '功',\n",
       " '能',\n",
       " '分',\n",
       " '别',\n",
       " '是',\n",
       " '什',\n",
       " '么',\n",
       " '？',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "83ba7b7d-bcec-42fa-8279-2fc1d91c11aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " None]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "3dd400fc-f0ce-4e94-8fd6-729d795d3289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "1dcad0b0-8e11-4048-8699-c0f7832674cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g',\n",
       " '2',\n",
       " '0',\n",
       " '标',\n",
       " '准',\n",
       " '版',\n",
       " '的',\n",
       " '进',\n",
       " '水',\n",
       " '压',\n",
       " '力',\n",
       " '最',\n",
       " '大',\n",
       " '值',\n",
       " '以',\n",
       " '及',\n",
       " 'p',\n",
       " '1',\n",
       " '0',\n",
       " 's',\n",
       " '标',\n",
       " '准',\n",
       " '版',\n",
       " '的',\n",
       " '洗',\n",
       " '衣',\n",
       " '热',\n",
       " '水',\n",
       " '功',\n",
       " '能',\n",
       " '分',\n",
       " '别',\n",
       " '是',\n",
       " '什',\n",
       " '么',\n",
       " '？']"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "5247d6e3-e678-4977-bf38-aea26384cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "b0225841-7dfc-44ea-9a19-63fb578f136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "3bf75f43-0abd-48a8-897c-ab3c4d08c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "07a2941b-d125-4bd1-a824-11e56eaad864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cc7491c0fe4653ac8beb423b2c67ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "0781e07d-6726-4e9a-a193-8302c58db343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1128\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 283\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "1bc04089-2b7d-4aab-a358-2c5011a45908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "054c807b-5934-4e07-be42-c422f2a7929c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    1,    2,    2,    2,\n",
       "            2,    2,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    1,    2,    2,    2,    2,    2,    0,    0,    0,    0,    0,\n",
       "            0, -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    2,\n",
       "            2,    2,    2,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    1,    2,    2,    2,    2,    2,    0,    0,    0,    0,    0,\n",
       "         -100, -100]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "4e9fec51-74f3-4ebe-9cf8-7c7538aad283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ec897e79-e002-4c42-ad1d-729b513856f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'I-name',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "d78998bc-6d44-4711-a995-1674d5838e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "ca6dd20e-b61e-41d0-81bd-8e5b5b656280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /data/dataset/huggingface/hub/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    '/data/dataset/huggingface/hub/bert-base-chinese',\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "b674de8f-6916-49da-ac70-b42617bd89c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "3bdd0c5d-6903-4ea0-b4a4-8694a2dbff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "d4c0aff2-e115-455b-927e-fde7fc7708e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "b3900b38-b624-4c6e-ba22-23aac20bd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    \"/workspace/data/private/zhuxiaohai/models/bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=8,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "95e7fd7f-97c5-4521-a283-bb7f6edf9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "318095a8-b292-4d33-9c88-8d3b36f78ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1128' max='1128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1128/1128 01:46, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.110872</td>\n",
       "      <td>0.770386</td>\n",
       "      <td>0.838785</td>\n",
       "      <td>0.803132</td>\n",
       "      <td>0.964120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.119376</td>\n",
       "      <td>0.813901</td>\n",
       "      <td>0.848131</td>\n",
       "      <td>0.830664</td>\n",
       "      <td>0.964765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.110834</td>\n",
       "      <td>0.800885</td>\n",
       "      <td>0.845794</td>\n",
       "      <td>0.822727</td>\n",
       "      <td>0.968637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.852804</td>\n",
       "      <td>0.831435</td>\n",
       "      <td>0.968895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.174527</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>0.855140</td>\n",
       "      <td>0.836571</td>\n",
       "      <td>0.969541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.184534</td>\n",
       "      <td>0.811530</td>\n",
       "      <td>0.855140</td>\n",
       "      <td>0.832765</td>\n",
       "      <td>0.969541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.189720</td>\n",
       "      <td>0.812918</td>\n",
       "      <td>0.852804</td>\n",
       "      <td>0.832383</td>\n",
       "      <td>0.969282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.190650</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.852804</td>\n",
       "      <td>0.831435</td>\n",
       "      <td>0.968895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1128, training_loss=0.04040920818951113, metrics={'train_runtime': 106.1903, 'train_samples_per_second': 84.98, 'train_steps_per_second': 10.622, 'total_flos': 225913502459088.0, 'train_loss': 0.04040920818951113, 'epoch': 8.0})"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "f4fe2a19-33c5-445e-b8a3-68304a3a4e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.19065029919147491,\n",
       " 'eval_precision': 0.8111111111111111,\n",
       " 'eval_recall': 0.852803738317757,\n",
       " 'eval_f1': 0.8314350797266513,\n",
       " 'eval_accuracy': 0.9688951987609705,\n",
       " 'eval_runtime': 1.0173,\n",
       " 'eval_samples_per_second': 278.174,\n",
       " 'eval_steps_per_second': 35.386,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "30695bb7-9ab4-45a5-8da4-dc8be228a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oot上面的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "0237bd56-f647-4896-bae7-2f2c58d96cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"/workspace/data/private/zhuxiaohai/models/bert_finetuned_ner_chinese/\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "526c338f-510c-42da-bc40-12022e49db0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-name',\n",
       "  'score': 0.99992514,\n",
       "  'index': 10,\n",
       "  'word': '污',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-name',\n",
       "  'score': 0.9998777,\n",
       "  'index': 11,\n",
       "  'word': '水',\n",
       "  'start': 12,\n",
       "  'end': 13},\n",
       " {'entity': 'I-name',\n",
       "  'score': 0.9998462,\n",
       "  'index': 12,\n",
       "  'word': '容',\n",
       "  'start': 13,\n",
       "  'end': 14},\n",
       " {'entity': 'I-name',\n",
       "  'score': 0.99978775,\n",
       "  'index': 13,\n",
       "  'word': '量',\n",
       "  'start': 14,\n",
       "  'end': 15}]"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier(\"\".join(raw_datasets['test'][\"tokens\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "5e07d4e9-be29-481a-a773-3eea02b28153",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = token_classifier(pd.Series(raw_datasets['test'][\"tokens\"]).apply(lambda x: \"\".join(x)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "7da03684-50be-494b-ac70-e10f406b9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "oot[\"set\"] = \"1train\"\n",
    "oot[\"pred\"] = np.nan\n",
    "oot['result'] = np.nan\n",
    "oot['result'] = oot['result'].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4cb1d309-3e63-488a-9c8b-4278a4956401",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(raw_datasets['test'].num_rows):\n",
    "    correct = 0\n",
    "    index = oot.index[int(raw_datasets['test'][i][\"index\"])]\n",
    "    oot.loc[index, 'set'] = '2test'\n",
    "    ner_list = oot.iloc[int(raw_datasets['test'][i][\"index\"])].ner_list[\"name\"]\n",
    "    question = oot.iloc[int(raw_datasets['test'][i][\"index\"])].question\n",
    "    if not ner_list:\n",
    "        if len(results[i]) == 0:\n",
    "            correct = -1\n",
    "    else:\n",
    "        for item in results[i]:\n",
    "            if question[item[\"start\"]:item[\"end\"]] in ner_list:\n",
    "                correct += 1 \n",
    "    oot.loc[index, 'pred'] = correct\n",
    "    oot.at[index, \"result\"] = [question[item[\"start\"]:item[\"end\"]] for item in results[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "cac530d2-534c-4032-b5cc-d60862ebbf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set\n",
       "1train    4276\n",
       "2test     1070\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot['set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "bc8aeff1-4e4c-46c9-ab03-0d94f99acdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred\n",
       "-1.0    467\n",
       " 0.0    106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot[(oot['set']=='2test')&(oot['ner_list'].apply(lambda x: len(x[\"name\"]))==0)][\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a1901044-293f-4775-8f00-e32b65075f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred\n",
       "1.0    264\n",
       "0.0    227\n",
       "2.0      6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot[(oot['set']=='2test')&(oot['ner_list'].apply(lambda x: len(x[\"name\"]))>0)][\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "919040ae-1544-41ab-995f-a37c25436a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred\n",
       "-1.0    0.436449\n",
       " 0.0    0.311215\n",
       " 1.0    0.246729\n",
       " 2.0    0.005607\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot[\"pred\"].value_counts(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "53e4f50b-0d3d-440a-963b-80882f94932e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ner_list</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>G10S主刷罩材质</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[主刷罩材质]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>G20清洁液不减少</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[清洁液]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>延边距离</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[延边距离]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>上下水指示灯不亮</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[上下水]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>扫地机运行宽度预留多少</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[运行宽度]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>为什么扫地机开启全屋清扫没有回洗过</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[清扫]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>主刷取不下</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[主刷]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>已经安装了，需要调整水管，师傅上门收费吗</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[水管]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>wifi5 需要开启备用网络么</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[络]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>G20设置了规避地毯，机器为什么还是过去清扫</td>\n",
       "      <td>{'name': {}}</td>\n",
       "      <td>[规避地毯]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    question      ner_list   result\n",
       "95                 G10S主刷罩材质  {'name': {}}  [主刷罩材质]\n",
       "110                G20清洁液不减少  {'name': {}}    [清洁液]\n",
       "177                     延边距离  {'name': {}}   [延边距离]\n",
       "198                 上下水指示灯不亮  {'name': {}}    [上下水]\n",
       "235              扫地机运行宽度预留多少  {'name': {}}   [运行宽度]\n",
       "...                      ...           ...      ...\n",
       "4927       为什么扫地机开启全屋清扫没有回洗过  {'name': {}}     [清扫]\n",
       "4994                   主刷取不下  {'name': {}}     [主刷]\n",
       "5001    已经安装了，需要调整水管，师傅上门收费吗  {'name': {}}     [水管]\n",
       "5076         wifi5 需要开启备用网络么  {'name': {}}      [络]\n",
       "5146  G20设置了规避地毯，机器为什么还是过去清扫  {'name': {}}   [规避地毯]\n",
       "\n",
       "[106 rows x 3 columns]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot[(oot['set']=='2test')&(oot['ner_list'].apply(lambda x: len(x[\"name\"]))==0)&(oot['pred']==0)][[\"question\", \"ner_list\", \"result\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0db2111b-4062-42da-8ea4-8717bd5ab52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义反查字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "9e8a7b1c6ff118d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:04.307794425Z",
     "start_time": "2024-04-30T06:57:04.285086613Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954\n"
     ]
    }
   ],
   "source": [
    "all_dict = {}\n",
    "for i in range(df_exploded.shape[0]):\n",
    "    replace = df_exploded['augment'].iloc[i][\"replace\"]\n",
    "    primaries = [j['primary_value'] for j in df_exploded['gen'].iloc[i]['prompt']]\n",
    "    for key in replace:\n",
    "        if replace[key] in primaries:\n",
    "            print(i)\n",
    "            continue\n",
    "        if key in all_dict:\n",
    "            all_dict[key].add(replace[key])\n",
    "        else:\n",
    "            all_dict[key] = set([replace[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "28411e60a0d55779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:22.736396566Z",
     "start_time": "2024-04-30T06:57:22.682318705Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in all_dict:\n",
    "    all_dict[key] = list(all_dict[key])\n",
    "    if key not in all_dict[key]:\n",
    "        all_dict[key] = all_dict[key] + [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "621dca5bf5452d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:23.519490303Z",
     "start_time": "2024-04-30T06:57:23.504082230Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cherry_pick(key, value):\n",
    "    if key not in all_dict:\n",
    "        print(key, 'oops')\n",
    "        return\n",
    "    u_list = all_dict[key]\n",
    "    for i in range(len(u_list)-1, -1, -1):\n",
    "        if u_list[i] == value:\n",
    "            u_list.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "a366952fa0f2c98e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:24.017445678Z",
     "start_time": "2024-04-30T06:57:23.999021453Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cherry_pick_list = []\n",
    "alias_inv_dict = {}\n",
    "for k, v in all_dict.items():\n",
    "    for u in v:\n",
    "        if u in alias_inv_dict:\n",
    "            cherry_pick_list.append((k, u, alias_inv_dict[u]))\n",
    "        alias_inv_dict[u] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "e4abe555ceb3b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:24.480460517Z",
     "start_time": "2024-04-30T06:57:24.459495761Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(cherry_pick_list)):\n",
    "    cherry_pick(cherry_pick_list[i][0], cherry_pick_list[i][1])\n",
    "    cherry_pick(cherry_pick_list[i][2], cherry_pick_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "450fe4b9f905d245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:25.452094423Z",
     "start_time": "2024-04-30T06:57:25.437066302Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cherry_pick_list = []\n",
    "alias_inv_dict = {}\n",
    "for k, v in all_dict.items():\n",
    "    for u in v:\n",
    "        if u in alias_inv_dict:\n",
    "            cherry_pick_list.append((k, u, alias_inv_dict[u]))\n",
    "        alias_inv_dict[u] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "ec934367d8067bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:25.895086217Z",
     "start_time": "2024-04-30T06:57:25.887697641Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cherry_pick_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "7ac24039d66a14ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:26.882459394Z",
     "start_time": "2024-04-30T06:57:26.845959699Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1796"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alias_inv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "cc010f33474efd9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:57:29.356787737Z",
     "start_time": "2024-04-30T06:57:28.654259187Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/dataset/kefu/all_entity.json']"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(all_dict, \"/data/dataset/kefu/all_entity.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b4718ff-bdeb-4d8e-8dfb-00bcbe6a0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实体提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "9e6dca3becaa54e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:58:25.031071234Z",
     "start_time": "2024-04-30T06:58:24.799985143Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_exploded[\"keywords\"] = df_exploded[\"question\"].apply(\n",
    "    lambda x: extract_keywords(x, all_model_list, wc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "a53ff9d723a0d0fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:58:25.723825075Z",
     "start_time": "2024-04-30T06:58:25.692538301Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'entity_extractor' from '/root/PycharmProjects/chatbot/entity_extractor.py'>"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib \n",
    "import database\n",
    "import preprocessing\n",
    "import entity_extractor\n",
    "importlib.reload(database)\n",
    "importlib.reload(preprocessing)\n",
    "importlib.reload(entity_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "7cdb9da42d124653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:58:28.582200748Z",
     "start_time": "2024-04-30T06:58:28.504953991Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ee = entity_extractor.EntityExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "9515b825f6918a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:58:30.708505413Z",
     "start_time": "2024-04-30T06:58:30.000241572Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_exploded[\"simple_entity\"] = df_exploded[\"question\"].apply(lambda x: ee.simple_match(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "7c25a1ac72baf352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:58:32.099528877Z",
     "start_time": "2024-04-30T06:58:31.361177202Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_exploded[\"entity\"] = df_exploded[\"keywords\"].apply(lambda x: ee.query_entity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "849484dc2280ecb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:58:32.771680905Z",
     "start_time": "2024-04-30T06:58:32.716926364Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>gen</th>\n",
       "      <th>template</th>\n",
       "      <th>type</th>\n",
       "      <th>final_prompt</th>\n",
       "      <th>augment</th>\n",
       "      <th>question</th>\n",
       "      <th>keywords</th>\n",
       "      <th>entity</th>\n",
       "      <th>simple_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sweeping</td>\n",
       "      <td>{'question': 'g20标准版的商品链接是什么？', 'prompt': [{'p...</td>\n",
       "      <td>[问询词0]的[关键词0]是什么？</td>\n",
       "      <td>gen_same_keywords_for_models</td>\n",
       "      <td>{\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...</td>\n",
       "      <td>{'sentence': '你能提供一下g20标准版的[商品访问链接]吗？', 'repla...</td>\n",
       "      <td>你能提供一下g20标准版的商品访问链接吗？</td>\n",
       "      <td>{'question': '你能提供一下g20标准版的商品访问链接吗？', 'model':...</td>\n",
       "      <td>[商品链接]</td>\n",
       "      <td>[商品链接]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sweeping</td>\n",
       "      <td>{'question': 'g20标准版的商品链接是什么？', 'prompt': [{'p...</td>\n",
       "      <td>[问询词0]的[关键词0]是什么？</td>\n",
       "      <td>gen_same_keywords_for_models</td>\n",
       "      <td>{\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...</td>\n",
       "      <td>{'sentence': 'g20标准版的[购买链接]可以获取吗？', 'replace':...</td>\n",
       "      <td>g20标准版的购买链接可以获取吗？</td>\n",
       "      <td>{'question': 'g20标准版的购买链接可以获取吗？', 'model': ['g...</td>\n",
       "      <td>[商品链接, 采购地]</td>\n",
       "      <td>[商品链接, 采购地]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sweeping</td>\n",
       "      <td>{'question': 'g20标准版的商品链接是什么？', 'prompt': [{'p...</td>\n",
       "      <td>[问询词0]的[关键词0]是什么？</td>\n",
       "      <td>gen_same_keywords_for_models</td>\n",
       "      <td>{\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...</td>\n",
       "      <td>{'sentence': '你知道g20标准版在哪里可以找到[在线购买页面链接]嘛？', '...</td>\n",
       "      <td>你知道g20标准版在哪里可以找到在线购买页面链接嘛？</td>\n",
       "      <td>{'question': '你知道g20标准版在哪里可以找到在线购买页面链接嘛？', 'mo...</td>\n",
       "      <td>[商品链接, 采购地]</td>\n",
       "      <td>[商品链接, 采购地]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sweeping</td>\n",
       "      <td>{'question': 'g20标准版的平台是什么？', 'prompt': [{'pri...</td>\n",
       "      <td>[问询词0]的[关键词0]是什么？</td>\n",
       "      <td>gen_same_keywords_for_models</td>\n",
       "      <td>{\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...</td>\n",
       "      <td>{'sentence': '您能告诉我g20标准版的平台是什么吗？', 'replace':...</td>\n",
       "      <td>您能告诉我g20标准版的平台是什么吗？</td>\n",
       "      <td>{'question': '您能告诉我g20标准版的平台是什么吗？', 'model': [...</td>\n",
       "      <td>[平台]</td>\n",
       "      <td>[平台]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sweeping</td>\n",
       "      <td>{'question': 'g20标准版的平台是什么？', 'prompt': [{'pri...</td>\n",
       "      <td>[问询词0]的[关键词0]是什么？</td>\n",
       "      <td>gen_same_keywords_for_models</td>\n",
       "      <td>{\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...</td>\n",
       "      <td>{'sentence': '你知道g20标准版所使用的[系统平台]是什么吗？', 'repl...</td>\n",
       "      <td>你知道g20标准版所使用的系统平台是什么吗？</td>\n",
       "      <td>{'question': '你知道g20标准版所使用的系统平台是什么吗？', 'model'...</td>\n",
       "      <td>[平台]</td>\n",
       "      <td>[平台]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat                                                gen  \\\n",
       "0  sweeping  {'question': 'g20标准版的商品链接是什么？', 'prompt': [{'p...   \n",
       "1  sweeping  {'question': 'g20标准版的商品链接是什么？', 'prompt': [{'p...   \n",
       "2  sweeping  {'question': 'g20标准版的商品链接是什么？', 'prompt': [{'p...   \n",
       "3  sweeping  {'question': 'g20标准版的平台是什么？', 'prompt': [{'pri...   \n",
       "4  sweeping  {'question': 'g20标准版的平台是什么？', 'prompt': [{'pri...   \n",
       "\n",
       "            template                          type  \\\n",
       "0  [问询词0]的[关键词0]是什么？  gen_same_keywords_for_models   \n",
       "1  [问询词0]的[关键词0]是什么？  gen_same_keywords_for_models   \n",
       "2  [问询词0]的[关键词0]是什么？  gen_same_keywords_for_models   \n",
       "3  [问询词0]的[关键词0]是什么？  gen_same_keywords_for_models   \n",
       "4  [问询词0]的[关键词0]是什么？  gen_same_keywords_for_models   \n",
       "\n",
       "                                        final_prompt  \\\n",
       "0  {\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...   \n",
       "1  {\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...   \n",
       "2  {\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...   \n",
       "3  {\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...   \n",
       "4  {\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...   \n",
       "\n",
       "                                             augment  \\\n",
       "0  {'sentence': '你能提供一下g20标准版的[商品访问链接]吗？', 'repla...   \n",
       "1  {'sentence': 'g20标准版的[购买链接]可以获取吗？', 'replace':...   \n",
       "2  {'sentence': '你知道g20标准版在哪里可以找到[在线购买页面链接]嘛？', '...   \n",
       "3  {'sentence': '您能告诉我g20标准版的平台是什么吗？', 'replace':...   \n",
       "4  {'sentence': '你知道g20标准版所使用的[系统平台]是什么吗？', 'repl...   \n",
       "\n",
       "                     question  \\\n",
       "0       你能提供一下g20标准版的商品访问链接吗？   \n",
       "1           g20标准版的购买链接可以获取吗？   \n",
       "2  你知道g20标准版在哪里可以找到在线购买页面链接嘛？   \n",
       "3         您能告诉我g20标准版的平台是什么吗？   \n",
       "4      你知道g20标准版所使用的系统平台是什么吗？   \n",
       "\n",
       "                                            keywords       entity  \\\n",
       "0  {'question': '你能提供一下g20标准版的商品访问链接吗？', 'model':...       [商品链接]   \n",
       "1  {'question': 'g20标准版的购买链接可以获取吗？', 'model': ['g...  [商品链接, 采购地]   \n",
       "2  {'question': '你知道g20标准版在哪里可以找到在线购买页面链接嘛？', 'mo...  [商品链接, 采购地]   \n",
       "3  {'question': '您能告诉我g20标准版的平台是什么吗？', 'model': [...         [平台]   \n",
       "4  {'question': '你知道g20标准版所使用的系统平台是什么吗？', 'model'...         [平台]   \n",
       "\n",
       "  simple_entity  \n",
       "0        [商品链接]  \n",
       "1   [商品链接, 采购地]  \n",
       "2   [商品链接, 采购地]  \n",
       "3          [平台]  \n",
       "4          [平台]  "
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "8183105e70d48704",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:41:49.263422084Z",
     "start_time": "2024-04-30T06:41:49.241597011Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['平台']"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"你知道g20标准版所使用的系统平台是什么吗？\"\n",
    "query_result = []\n",
    "for name in ee.all_entity:\n",
    "    if (name == query) or (query in name) or (name in query):\n",
    "        query_result.append(name)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "ac6e369de248ec18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:41:49.731764754Z",
     "start_time": "2024-04-30T06:41:49.682207178Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g20标准版 清扫路线\n",
      "系统平台 平台\n",
      "平台 平台\n"
     ]
    }
   ],
   "source": [
    "for alias in sorted(ee.entity_inv.keys(), key=lambda k: len(k), reverse=True):\n",
    "    if (alias == query) or (query in alias) or (alias in query):\n",
    "        print(alias, ee.entity_inv[alias])\n",
    "        query_result.append(ee.entity_inv[alias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "3a23c519250ff295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:43:49.975882442Z",
     "start_time": "2024-04-30T06:43:49.944112030Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"g20标准版\" in ee.entity_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e7be3a99ad84ad17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:13:49.122466584Z",
     "start_time": "2024-04-29T11:13:49.092692233Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "import copy\n",
    "from database import DataQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "2033fb227aca7f54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:13:50.014424953Z",
     "start_time": "2024-04-29T11:13:50.008637044Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def row_to_sentence_simple_query(row):\n",
    "    other_data = {k: v for k, v in row.items() if k not in [\n",
    "        '版本', '商品型号']}\n",
    "    details = []\n",
    "    for col, value in other_data.items():\n",
    "        if pd.notna(value):\n",
    "            detail = f\"{col}为{value}\"\n",
    "            details.append(detail)\n",
    "    return '; '.join(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "dcc9952988322e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T06:08:30.247358119Z",
     "start_time": "2024-04-30T06:08:30.184062805Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'entity_extractor' from '/root/PycharmProjects/chatbot/entity_extractor.py'>"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "41d082ba77dffef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T13:07:09.089615088Z",
     "start_time": "2024-04-29T13:07:09.039239265Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat                                                      sweeping\n",
       "gen             {'question': 'g20标准版的商品链接是什么？', 'prompt': [{'p...\n",
       "template                                        [问询词0]的[关键词0]是什么？\n",
       "type                                 gen_same_keywords_for_models\n",
       "final_prompt    {\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...\n",
       "augment         {'sentence': '你能提供一下g20标准版的[商品访问链接]吗？', 'repla...\n",
       "question                                    你能提供一下g20标准版的商品访问链接吗？\n",
       "keywords        {'model': ['g20'], 'version': ['标准版'], 'keywor...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "3868a3fd40c3927d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:31:48.003080954Z",
     "start_time": "2024-04-29T11:31:47.953882590Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建 DataQuery 的实例\n",
    "dq = database.DataQuery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "1dba1a0ce2498383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:31:49.027626836Z",
     "start_time": "2024-04-29T11:31:49.015406398Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def handler_simple_query(question_obj):\n",
    "    model = question_obj['model'][0]\n",
    "    keywords = question_obj['keywords']\n",
    "    try:\n",
    "        version = question_obj['version'][0]\n",
    "    except:\n",
    "        version = \"\"\n",
    "\n",
    "    all_data = dq.query_data(model)\n",
    "    answer_list = []\n",
    "    find_list = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        ori_keyword = keyword\n",
    "        if keyword not in all_data.columns:\n",
    "            keyword, _ = find_best_match_new(\n",
    "                keyword, all_data.columns, threshold=65)\n",
    "\n",
    "        if keyword in all_data.columns and keyword not in ['版本', '商品型号']:\n",
    "            find_list.append(keyword)\n",
    "            tmp_df = copy.deepcopy(all_data[['商品型号', '版本', keyword]])\n",
    "            tmp_df = tmp_df.loc[(tmp_df['商品型号'] == model)]\n",
    "            if version:\n",
    "                tmp_df = tmp_df.loc[(tmp_df['版本'] == version)]\n",
    "            if tmp_df.empty:\n",
    "                return ''\n",
    "            sentences = tmp_df.apply(row_to_sentence_simple_query, axis=1)\n",
    "            sentences = '\\n'.join(sentences)\n",
    "            if ori_keyword != keyword:\n",
    "                sentences = sentences.replace(keyword, f'{ori_keyword}({keyword})')\n",
    "            if len(sentences) > 0:\n",
    "                answer_list.append(sentences)\n",
    "    \n",
    "    details = '，'.join(answer_list)\n",
    "    answer = f'{model}{version}的{details}'\n",
    "    \n",
    "    return answer, find_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "29d19ca528a1d300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:31:49.890337454Z",
     "start_time": "2024-04-29T11:31:49.881538798Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_best_match_new(question, mapping_list, threshold=65):\n",
    "    matches = process.extract(question, mapping_list)\n",
    "    best_matches = [match for match in matches if match[1] >= threshold]\n",
    "    best_matches = sorted(best_matches, key=lambda x: (-x[1], -len(x[0])))\n",
    "    # print(best_matches)\n",
    "    match_score = 0\n",
    "    total_q = \"\"\n",
    "    if len(best_matches) > 0:\n",
    "        total_q = best_matches[0][0]\n",
    "        match_score = best_matches[0][1]\n",
    "\n",
    "    return total_q, match_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "248d899caf42808a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T04:16:19.090141146Z",
     "start_time": "2024-04-23T04:16:19.083437632Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ['p10pro'], 'version': ['上下水版'], 'keywords': ['清洁液', '毫升']}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_obj = oot[oot.version_keywords.apply(lambda x: len(x)>0)][\"keywords\"].iloc[0]\n",
    "question_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a2854e3a52262e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T04:16:19.443175002Z",
     "start_time": "2024-04-23T04:16:19.433769321Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p10pro上下水版的清洁液(清洁液储存盒)为1000ml'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler_simple_query(question_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "a6e6351675ac7b93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:24:47.859941829Z",
     "start_time": "2024-04-29T11:24:47.838531053Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"平台\" in dq.all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "cf9240e11793205a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:45:25.788931580Z",
     "start_time": "2024-04-29T11:45:25.747139126Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('g20标准版的商品(商品编码)为4969634692760，链接(商品链接)为https://detail.tmall.com/item.htm?id=707235140054&skuId=4969634692760&spm=a21dvs.23580594.0.0.3f063d0d9Hg2Xc',\n",
       " ['商品编码', '商品链接'])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler_simple_query(df_exploded[\"keywords\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "fd0ae80fe607b432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:32:26.178143126Z",
     "start_time": "2024-04-29T11:32:26.129378127Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ['h1'], 'version': [], 'keywords': ['使用', '平台', '请问']}"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded[\"keywords\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "88f74041cb241caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:45:30.991585156Z",
     "start_time": "2024-04-29T11:45:30.979768140Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat                                                      sweeping\n",
       "gen             {'question': 'g20标准版的商品链接是什么？', 'prompt': [{'p...\n",
       "template                                        [问询词0]的[关键词0]是什么？\n",
       "type                                 gen_same_keywords_for_models\n",
       "final_prompt    {\"template\": \"[问询词0]的[关键词0]是什么？\", \"replace\": {...\n",
       "augment         {'sentence': '你能提供一下g20标准版的[商品访问链接]吗？', 'repla...\n",
       "question                                    你能提供一下g20标准版的商品访问链接吗？\n",
       "keywords        {'model': ['g20'], 'version': ['标准版'], 'keywor...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exploded.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "5fa064dad8e5b785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:31:24.162443992Z",
     "start_time": "2024-04-29T11:31:24.104580195Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_obj = df_exploded[\"keywords\"].iloc[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "eda87db3ae978616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:31:24.636921840Z",
     "start_time": "2024-04-29T11:31:24.624198017Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ['h1'], 'version': [], 'keywords': ['使用', '平台', '请问']}"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "8cb62065d7a3c318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:28:45.687951915Z",
     "start_time": "2024-04-29T11:28:45.676148844Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = question_obj['model'][0]\n",
    "keywords = question_obj['keywords']\n",
    "try:\n",
    "    version = question_obj['version'][0]\n",
    "except:\n",
    "    version = \"\"\n",
    "\n",
    "all_data = dq.query_data(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "1e865840a1eabc78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:28:50.903237786Z",
     "start_time": "2024-04-29T11:28:50.896361368Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>版本</th>\n",
       "      <th>商品编码</th>\n",
       "      <th>平台ID</th>\n",
       "      <th>商品id</th>\n",
       "      <th>商品型号</th>\n",
       "      <th>商品名字</th>\n",
       "      <th>商品分类</th>\n",
       "      <th>商品链接</th>\n",
       "      <th>平台</th>\n",
       "      <th>店铺名称</th>\n",
       "      <th>...</th>\n",
       "      <th>内筒照明</th>\n",
       "      <th>断电记忆</th>\n",
       "      <th>童锁功能</th>\n",
       "      <th>中途添衣</th>\n",
       "      <th>紧急开门</th>\n",
       "      <th>机门锁定/解锁</th>\n",
       "      <th>额定洗涤输入功率</th>\n",
       "      <th>额定脱水输入功率</th>\n",
       "      <th>额定加热输入功率</th>\n",
       "      <th>防水等级</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>标准版</td>\n",
       "      <td>4969634692760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4969634692760</td>\n",
       "      <td>G20</td>\n",
       "      <td>石头自清洁扫地机器人G20系列扫拖地全自动上下水家用清洗一体机</td>\n",
       "      <td>扫地机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=707235140...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>标准版</td>\n",
       "      <td>5268670697517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5268670697517</td>\n",
       "      <td>P10S</td>\n",
       "      <td>【新品上市】石头自清洁扫地机器人P10S系列全自动扫拖地清洗一体</td>\n",
       "      <td>扫地机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=766171501...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>上下水版</td>\n",
       "      <td>5439727679635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5439727679635</td>\n",
       "      <td>P10S Pro</td>\n",
       "      <td>【新品上市】石头自清洁扫地机器人P10S Pro系列扫拖一体全自动</td>\n",
       "      <td>扫地机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=766696298...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>标准版</td>\n",
       "      <td>512525013970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512525013970</td>\n",
       "      <td>G10S Pure</td>\n",
       "      <td>石头自清洁扫地机器人G10S Pure系列扫拖一体机全自动上下水家用</td>\n",
       "      <td>扫地机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=670141190...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>标准版</td>\n",
       "      <td>4952229349955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4952229349955</td>\n",
       "      <td>G10S Pro</td>\n",
       "      <td>石头自清洁扫拖机器人G10S系列全自动家用扫地拖地吸尘三合一体机</td>\n",
       "      <td>扫地机</td>\n",
       "      <td>https://detail.tmall.com/item.htm?id=672078527...</td>\n",
       "      <td>淘宝</td>\n",
       "      <td>石头电器旗舰店</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     版本           商品编码  平台ID           商品id       商品型号  \\\n",
       "0   标准版  4969634692760   NaN  4969634692760        G20   \n",
       "1   标准版  5268670697517   NaN  5268670697517       P10S   \n",
       "2  上下水版  5439727679635   NaN  5439727679635   P10S Pro   \n",
       "3   标准版   512525013970   NaN   512525013970  G10S Pure   \n",
       "4   标准版  4952229349955   NaN  4952229349955   G10S Pro   \n",
       "\n",
       "                                 商品名字 商品分类  \\\n",
       "0     石头自清洁扫地机器人G20系列扫拖地全自动上下水家用清洗一体机  扫地机   \n",
       "1    【新品上市】石头自清洁扫地机器人P10S系列全自动扫拖地清洗一体  扫地机   \n",
       "2   【新品上市】石头自清洁扫地机器人P10S Pro系列扫拖一体全自动  扫地机   \n",
       "3  石头自清洁扫地机器人G10S Pure系列扫拖一体机全自动上下水家用  扫地机   \n",
       "4    石头自清洁扫拖机器人G10S系列全自动家用扫地拖地吸尘三合一体机  扫地机   \n",
       "\n",
       "                                                商品链接  平台     店铺名称  ...  内筒照明  \\\n",
       "0  https://detail.tmall.com/item.htm?id=707235140...  淘宝  石头电器旗舰店  ...   NaN   \n",
       "1  https://detail.tmall.com/item.htm?id=766171501...  淘宝  石头电器旗舰店  ...   NaN   \n",
       "2  https://detail.tmall.com/item.htm?id=766696298...  淘宝  石头电器旗舰店  ...   NaN   \n",
       "3  https://detail.tmall.com/item.htm?id=670141190...  淘宝  石头电器旗舰店  ...   NaN   \n",
       "4  https://detail.tmall.com/item.htm?id=672078527...  淘宝  石头电器旗舰店  ...   NaN   \n",
       "\n",
       "  断电记忆 童锁功能 中途添衣 紧急开门 机门锁定/解锁 额定洗涤输入功率 额定脱水输入功率 额定加热输入功率 防水等级  \n",
       "0  NaN  NaN  NaN  NaN     NaN      NaN      NaN      NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN     NaN      NaN      NaN      NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN     NaN      NaN      NaN      NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN     NaN      NaN      NaN      NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN     NaN      NaN      NaN      NaN  NaN  \n",
       "\n",
       "[5 rows x 210 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87a4a44ef73d9b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = question_obj['model'][0]\n",
    "keywords = question_obj['keywords']\n",
    "try:\n",
    "    version = question_obj['version'][0]\n",
    "except:\n",
    "    version = \"\"\n",
    "\n",
    "all_data = dq.query_data(model)\n",
    "answer_list = []\n",
    "find_list = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    ori_keyword = keyword\n",
    "    if keyword not in all_data.columns:\n",
    "        keyword, _ = find_best_match_new(\n",
    "            keyword, all_data.columns, threshold=65)\n",
    "\n",
    "    if keyword in all_data.columns and keyword not in ['版本', '商品型号']:\n",
    "        find_list.append(keyword)\n",
    "        tmp_df = copy.deepcopy(all_data[['商品型号', '版本', keyword]])\n",
    "        tmp_df = tmp_df.loc[(tmp_df['商品型号'] == model)]\n",
    "        if version:\n",
    "            tmp_df = tmp_df.loc[(tmp_df['版本'] == version)]\n",
    "        sentences = tmp_df.apply(row_to_sentence_simple_query, axis=1)\n",
    "        sentences = '\\n'.join(sentences)\n",
    "        if ori_keyword != keyword:\n",
    "            sentences = sentences.replace(keyword, f'{ori_keyword}({keyword})')\n",
    "        if len(sentences) > 0:\n",
    "            answer_list.append(sentences)\n",
    "details = '，'.join(answer_list)\n",
    "answer = f'{model}{version}的{details}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "cd00de7dd451d29d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:22:10.278683500Z",
     "start_time": "2024-04-29T11:21:41.544065617Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n"
     ]
    }
   ],
   "source": [
    "for i in range(df_exploded.shape[0]):\n",
    "    print(i)\n",
    "    a = handler_simple_query(df_exploded[\"keywords\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "750dcda5caa5cea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T11:13:55.451697606Z",
     "start_time": "2024-04-29T11:13:55.314983590Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[316], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_exploded[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf_exploded\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkeywords\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandler_simple_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/chatbot/lib/python3.10/site-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/chatbot/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/chatbot/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/mambaforge/envs/chatbot/lib/python3.10/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mambaforge/envs/chatbot/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[316], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_exploded[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_exploded[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeywords\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mhandler_simple_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[0;31mIndexError\u001B[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "df_exploded['result'] = df_exploded[\"keywords\"].apply(lambda x: handler_simple_query(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ad05da620b853",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
